{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install core dependencies with compatible versions\n",
        "\n",
        "print(\"Installing dependencies...\")\n",
        "\n",
        "# Install base packages\n",
        "%uv pip install -q psutil setuptools\n",
        "\n",
        "# Install compatible transformers and peft versions\n",
        "%uv pip install -q transformers==4.57.3\n",
        "%uv pip install -q \"peft==0.18.0\"\n",
        "%uv pip install -q \"bitsandbytes>=0.44.1\"\n",
        "%uv pip install -q torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu124\n",
        "\n",
        "# Install flash attention\n",
        "%uv pip install flash-attn==2.7.4.post1 --no-build-isolation\n",
        "\n",
        "# Install GRPO-specific dependencies\n",
        "%uv pip install -q \"ray[default]==2.44.0\"  # Ray for distributed training\n",
        "%uv pip install -q vllm==0.8.4  # vLLM for efficient generation\n",
        "%uv pip install -q omegaconf==2.3.0 jinja2  # Config and template\n",
        "\n",
        "# Install other requirements\n",
        "%uv pip install -q accelerate==1.10.1\n",
        "%uv pip install -q mmengine==0.10.7 ujson==5.11.0\n",
        "\n",
        "# Install vision dependencies\n",
        "%uv pip install -q Pillow pandas matplotlib numpy tqdm fastparquet pyarrow\n",
        "%uv pip install -q qwen-vl-utils pycocotools\n",
        "\n",
        "# Install liger-kernel\n",
        "%uv pip install -q liger-kernel\n",
        "\n",
        "print(\"\\n✓ Dependencies installed!\")\n",
        "\n",
        "import transformers\n",
        "import ray\n",
        "print(f\"✓ Transformers: {transformers.__version__}\")\n",
        "print(f\"✓ Ray: {ray.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Determine project root\n",
        "PROJECT_ROOT = Path.cwd()\n",
        "while PROJECT_ROOT.name != 'Rex-Omni' and PROJECT_ROOT.parent != PROJECT_ROOT:\n",
        "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
        "if PROJECT_ROOT.name != 'Rex-Omni':\n",
        "    PROJECT_ROOT = Path.cwd()\n",
        "os.chdir(PROJECT_ROOT)\n",
        "\n",
        "# Add finetuning to path\n",
        "FINETUNING_PATH = PROJECT_ROOT / 'finetuning'\n",
        "for p in [str(PROJECT_ROOT), str(FINETUNING_PATH)]:\n",
        "    if p not in sys.path:\n",
        "        sys.path.insert(0, p)\n",
        "\n",
        "print(f\"✓ Project root: {PROJECT_ROOT}\")\n",
        "print(f\"✓ Finetuning path: {FINETUNING_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core imports\n",
        "import json\n",
        "import base64\n",
        "import io\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "print(f\"✓ PyTorch: {torch.__version__}\")\n",
        "print(f\"✓ CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"✓ GPU count: {torch.cuda.device_count()}\")\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== MODEL ====================\n",
        "MODEL_NAME = \"IDEA-Research/Rex-Omni\"\n",
        "\n",
        "# ==================== DATA ====================\n",
        "VRSBENCH_PARQUET = PROJECT_ROOT / \"vrsbench_val_data.parquet\"\n",
        "VRSBENCH_IMAGES_DIR = PROJECT_ROOT / \"Images_validation\" / \"Images_val\"  # Update this!\n",
        "TSV_OUTPUT_DIR = PROJECT_ROOT / \"data\" / \"vrsbench_grpo\"\n",
        "NUM_SAMPLES = 1000  # Number of samples for GRPO training\n",
        "\n",
        "# Image processing (same as SFT)\n",
        "MIN_PIXELS = 16 * 28 * 28   # 12,544\n",
        "MAX_PIXELS = 2560 * 28 * 28  # 2,007,040\n",
        "\n",
        "# ==================== GRPO TRAINING ====================\n",
        "OUTPUT_DIR = PROJECT_ROOT / \"work_dirs\" / \"grpo_vrsbench\"\n",
        "N_GPUS = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
        "\n",
        "# GRPO hyperparameters\n",
        "ROLLOUT_N = 8  # Number of responses per prompt\n",
        "GLOBAL_BATCH_SIZE = min(64, NUM_SAMPLES // 4)\n",
        "ROLLOUT_BATCH_SIZE = min(64, NUM_SAMPLES // 4)\n",
        "LEARNING_RATE = 1e-6\n",
        "KL_COEF = 0.01\n",
        "TOTAL_EPOCHS = 1\n",
        "\n",
        "print(\"GRPO Configuration:\")\n",
        "print(f\"  Model: {MODEL_NAME}\")\n",
        "print(f\"  Samples: {NUM_SAMPLES}\")\n",
        "print(f\"  GPUs: {N_GPUS}\")\n",
        "print(f\"  Rollout N: {ROLLOUT_N} (responses per prompt)\")\n",
        "print(f\"  Global batch size: {GLOBAL_BATCH_SIZE}\")\n",
        "print(f\"  Output: {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Convert VRSBench to TSV Format (Same as SFT)\n",
        "\n",
        "The GRPO training uses the same TSV format as SFT training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_vrsbench_to_tsv(\n",
        "    parquet_path: Path,\n",
        "    images_dir: Path,\n",
        "    output_dir: Path,\n",
        "    num_samples: int = 1000,\n",
        "    project_root: Path = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Convert VRSBench parquet to TSV format for GRPO training.\n",
        "    Same format as SFT - used by TSVRLHFDataset.\n",
        "    \"\"\"\n",
        "    output_dir = Path(output_dir)\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Output file paths\n",
        "    img_tsv = output_dir / \"train.images.tsv\"\n",
        "    ann_tsv = output_dir / \"train.annotations.tsv\"\n",
        "    ann_idx = output_dir / \"train.annotations.tsv.lineidx\"\n",
        "    \n",
        "    print(f\"Loading {parquet_path}...\")\n",
        "    df = pd.read_parquet(parquet_path)\n",
        "    print(f\"  Total samples: {len(df)}\")\n",
        "    \n",
        "    df_sample = df.head(num_samples).copy()\n",
        "    print(f\"  Using: {len(df_sample)} samples\")\n",
        "    \n",
        "    converted = 0\n",
        "    skipped = 0\n",
        "    errors = []\n",
        "    \n",
        "    img_offset = 0\n",
        "    ann_offset = 0\n",
        "    \n",
        "    with open(img_tsv, 'w', encoding='utf-8') as f_img, \\\n",
        "         open(ann_tsv, 'w', encoding='utf-8') as f_ann, \\\n",
        "         open(ann_idx, 'w', encoding='utf-8') as f_idx:\n",
        "        \n",
        "        for idx, row in tqdm(df_sample.iterrows(), total=len(df_sample), desc=\"Converting\"):\n",
        "            try:\n",
        "                # Find image\n",
        "                image_path = row.get('image_path', '')\n",
        "                if not image_path:\n",
        "                    skipped += 1\n",
        "                    continue\n",
        "                \n",
        "                img_path = None\n",
        "                candidates = [\n",
        "                    Path(image_path),\n",
        "                    images_dir / Path(image_path).name,\n",
        "                    project_root / image_path.lstrip('./') if project_root else None,\n",
        "                ]\n",
        "                for candidate in candidates:\n",
        "                    if candidate and candidate.exists():\n",
        "                        img_path = candidate\n",
        "                        break\n",
        "                \n",
        "                if img_path is None:\n",
        "                    if len(errors) < 5:\n",
        "                        errors.append(f\"Image not found: {image_path}\")\n",
        "                    skipped += 1\n",
        "                    continue\n",
        "                \n",
        "                # Load and encode image\n",
        "                pil_img = Image.open(img_path).convert('RGB')\n",
        "                img_width, img_height = pil_img.size\n",
        "                \n",
        "                buffer = io.BytesIO()\n",
        "                pil_img.save(buffer, format='JPEG', quality=95)\n",
        "                img_b64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
        "                \n",
        "                # Parse objects\n",
        "                objects_data = row.get('objects')\n",
        "                if objects_data is None:\n",
        "                    skipped += 1\n",
        "                    continue\n",
        "                \n",
        "                if isinstance(objects_data, str):\n",
        "                    objects_list = json.loads(objects_data)\n",
        "                else:\n",
        "                    objects_list = objects_data\n",
        "                \n",
        "                if not isinstance(objects_list, list) or len(objects_list) == 0:\n",
        "                    skipped += 1\n",
        "                    continue\n",
        "                \n",
        "                # Build boxes\n",
        "                boxes = []\n",
        "                for obj in objects_list:\n",
        "                    if not isinstance(obj, dict):\n",
        "                        continue\n",
        "                    \n",
        "                    bbox_norm = obj.get('obj_coord')\n",
        "                    if not bbox_norm or len(bbox_norm) != 4:\n",
        "                        continue\n",
        "                    \n",
        "                    # Convert normalized to absolute coordinates\n",
        "                    x0 = bbox_norm[0] * img_width\n",
        "                    y0 = bbox_norm[1] * img_height\n",
        "                    x1 = bbox_norm[2] * img_width\n",
        "                    y1 = bbox_norm[3] * img_height\n",
        "                    \n",
        "                    phrase = obj.get('referring_sentence', '') or obj.get('obj_cls', '')\n",
        "                    if not phrase:\n",
        "                        continue\n",
        "                    \n",
        "                    boxes.append({'bbox': [x0, y0, x1, y1], 'phrase': str(phrase)})\n",
        "                \n",
        "                if len(boxes) == 0:\n",
        "                    skipped += 1\n",
        "                    continue\n",
        "                \n",
        "                # Write to TSV\n",
        "                f_idx.write(f\"{ann_offset}\\n\")\n",
        "                \n",
        "                img_line = f\"{img_offset}\\t{img_b64}\\n\"\n",
        "                f_img.write(img_line)\n",
        "                img_line_bytes = len(img_line.encode('utf-8'))\n",
        "                \n",
        "                ann_json = json.dumps({\"boxes\": boxes}, ensure_ascii=False)\n",
        "                ann_line = f\"{img_offset}\\t{ann_json}\\n\"\n",
        "                f_ann.write(ann_line)\n",
        "                ann_line_bytes = len(ann_line.encode('utf-8'))\n",
        "                \n",
        "                img_offset += img_line_bytes\n",
        "                ann_offset += ann_line_bytes\n",
        "                converted += 1\n",
        "                \n",
        "            except Exception as e:\n",
        "                if len(errors) < 5:\n",
        "                    errors.append(f\"Row {idx}: {str(e)}\")\n",
        "                skipped += 1\n",
        "    \n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Conversion Complete!\")\n",
        "    print(f\"  ✓ Converted: {converted}\")\n",
        "    print(f\"  ✗ Skipped: {skipped}\")\n",
        "    \n",
        "    if errors:\n",
        "        print(f\"\\nErrors:\")\n",
        "        for err in errors:\n",
        "            print(f\"  - {err}\")\n",
        "    \n",
        "    if converted == 0:\n",
        "        raise RuntimeError(f\"No samples converted! Check image paths.\")\n",
        "    \n",
        "    return converted\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run conversion\n",
        "if not VRSBENCH_IMAGES_DIR.exists():\n",
        "    print(f\"⚠️  VRSBench images not found: {VRSBENCH_IMAGES_DIR}\")\n",
        "    print(\"Please update VRSBENCH_IMAGES_DIR in configuration.\")\n",
        "else:\n",
        "    num_converted = convert_vrsbench_to_tsv(\n",
        "        parquet_path=VRSBENCH_PARQUET,\n",
        "        images_dir=VRSBENCH_IMAGES_DIR,\n",
        "        output_dir=TSV_OUTPUT_DIR,\n",
        "        num_samples=NUM_SAMPLES,\n",
        "        project_root=PROJECT_ROOT\n",
        "    )\n",
        "    print(f\"\\n✓ Ready for GRPO training with {num_converted} samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Create GRPO Configuration\n",
        "\n",
        "GRPO uses a Python config file that specifies dataset and task function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create GRPO config file for VRSBench\n",
        "grpo_config_content = f'''\"\"\"GRPO Config for VRSBench Dataset\"\"\"\n",
        "from dataset.task_fns import GroundingTaskFn\n",
        "from dataset.task_fns.task_prompts.grounding_task import GROUNDING_SINGLE_REGION_STAGE_XYXY\n",
        "from verl.utils.dataset import TSVRLHFDataset\n",
        "\n",
        "min_pixels = {MIN_PIXELS}\n",
        "max_pixels = {MAX_PIXELS}\n",
        "\n",
        "grounding_data = dict(\n",
        "    type=TSVRLHFDataset,\n",
        "    image_tsv_file=\"{TSV_OUTPUT_DIR / 'train.images.tsv'}\",\n",
        "    anno_tsv_file=\"{TSV_OUTPUT_DIR / 'train.annotations.tsv'}\",\n",
        "    anno_idx_file=\"{TSV_OUTPUT_DIR / 'train.annotations.tsv.lineidx'}\",\n",
        "    min_pixels=min_pixels,\n",
        "    max_pixels=max_pixels,\n",
        "    task_fn=dict(\n",
        "        type=GroundingTaskFn,\n",
        "        task_prompts=GROUNDING_SINGLE_REGION_STAGE_XYXY,\n",
        "        image_min_pixels=min_pixels,\n",
        "        image_max_pixels=max_pixels,\n",
        "    ),\n",
        "    dataset_name=\"vrsbench_grounding\",\n",
        "    reward_name=\"box_iou\",  # Uses IoU-based reward\n",
        ")\n",
        "\n",
        "train_dataset = [\n",
        "    grounding_data,\n",
        "]\n",
        "'''\n",
        "\n",
        "# Write config file\n",
        "config_dir = FINETUNING_PATH / \"configs\"\n",
        "config_dir.mkdir(parents=True, exist_ok=True)\n",
        "config_path = config_dir / \"grpo_vrsbench.py\"\n",
        "\n",
        "with open(config_path, 'w') as f:\n",
        "    f.write(grpo_config_content)\n",
        "\n",
        "print(f\"✓ GRPO config written to: {config_path}\")\n",
        "print(\"\\nConfig contents:\")\n",
        "print(\"-\" * 50)\n",
        "print(grpo_config_content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Launch GRPO Training\n",
        "\n",
        "GRPO training uses Ray for distributed training. There are two ways to run it:\n",
        "\n",
        "**Option A**: Run via shell command (recommended for multi-GPU)\n",
        "**Option B**: Run directly in notebook (for debugging)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate the training command\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "training_command = f'''cd {FINETUNING_PATH} && \\\\\n",
        "python3 -m verl.trainer.main \\\\\n",
        "    config=verl/configs/config.yaml \\\\\n",
        "    data.config_path=\"configs/grpo_vrsbench.py\" \\\\\n",
        "    data.format_prompt=verl/configs/r1v_format.jinja \\\\\n",
        "    worker.actor.model.model_path={MODEL_NAME} \\\\\n",
        "    trainer.experiment_name=\"grpo_vrsbench\" \\\\\n",
        "    trainer.n_gpus_per_node={N_GPUS} \\\\\n",
        "    worker.actor.global_batch_size={GLOBAL_BATCH_SIZE} \\\\\n",
        "    data.rollout_batch_size={ROLLOUT_BATCH_SIZE} \\\\\n",
        "    worker.actor.micro_batch_size_per_device_for_update=2 \\\\\n",
        "    worker.actor.micro_batch_size_per_device_for_experience=4 \\\\\n",
        "    worker.rollout.n={ROLLOUT_N} \\\\\n",
        "    worker.rollout.tensor_parallel_size={min(2, N_GPUS)} \\\\\n",
        "    algorithm.kl_coef={KL_COEF} \\\\\n",
        "    worker.actor.optim.lr={LEARNING_RATE} \\\\\n",
        "    trainer.total_epochs={TOTAL_EPOCHS} \\\\\n",
        "    trainer.save_checkpoint_path=\"{OUTPUT_DIR}\" \\\\\n",
        "    trainer.save_freq=50 \\\\\n",
        "    trainer.logger=\"[\\\\\"console\\\\\"]\"\n",
        "'''\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"OPTION A: Run via Terminal (Recommended)\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nCopy and run this command in your terminal:\\n\")\n",
        "print(training_command)\n",
        "\n",
        "# Also save as shell script\n",
        "script_path = PROJECT_ROOT / \"run_grpo_vrsbench.sh\"\n",
        "with open(script_path, 'w') as f:\n",
        "    f.write(\"#!/bin/bash\\n\")\n",
        "    f.write(f\"export OUTPUT_PATH=\\\"{OUTPUT_DIR}\\\"\\n\")\n",
        "    f.write(\"export DEBUG_MODE=\\\"true\\\"\\n\")\n",
        "    f.write(\"export PYTHONUNBUFFERED=1\\n\")\n",
        "    f.write(\"export RAY_DISABLE_IMPORT_WARNING=1\\n\\n\")\n",
        "    f.write(training_command.replace(\" \\\\\\n\", \" \\\\\\n    \"))\n",
        "\n",
        "print(f\"\\n✓ Shell script saved to: {script_path}\")\n",
        "print(f\"  Run with: bash {script_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option B: Run Training from Notebook\n",
        "\n",
        "⚠️ **Warning**: This requires multiple GPUs and significant memory. Only run if you have the hardware.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# OPTION B: Run training directly (uncomment to run)\n",
        "# This cell launches GRPO training - requires multi-GPU setup\n",
        "\n",
        "RUN_TRAINING = False  # Set to True to run training\n",
        "\n",
        "if RUN_TRAINING:\n",
        "    import subprocess\n",
        "    import os\n",
        "    \n",
        "    # Set environment variables\n",
        "    env = os.environ.copy()\n",
        "    env[\"OUTPUT_PATH\"] = str(OUTPUT_DIR)\n",
        "    env[\"DEBUG_MODE\"] = \"true\"\n",
        "    env[\"PYTHONUNBUFFERED\"] = \"1\"\n",
        "    env[\"RAY_DISABLE_IMPORT_WARNING\"] = \"1\"\n",
        "    env[\"RAY_ADDRESS\"] = \"\"\n",
        "    env[\"RAY_CLIENT_MODE\"] = \"\"\n",
        "    \n",
        "    # Change to finetuning directory\n",
        "    os.chdir(FINETUNING_PATH)\n",
        "    \n",
        "    print(\"Starting GRPO training...\")\n",
        "    print(f\"Output directory: {OUTPUT_DIR}\")\n",
        "    print(f\"GPUs: {N_GPUS}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Run training\n",
        "    cmd = [\n",
        "        \"python3\", \"-m\", \"verl.trainer.main\",\n",
        "        \"config=verl/configs/config.yaml\",\n",
        "        f\"data.config_path=configs/grpo_vrsbench.py\",\n",
        "        \"data.format_prompt=verl/configs/r1v_format.jinja\",\n",
        "        f\"worker.actor.model.model_path={MODEL_NAME}\",\n",
        "        \"trainer.experiment_name=grpo_vrsbench\",\n",
        "        f\"trainer.n_gpus_per_node={N_GPUS}\",\n",
        "        f\"worker.actor.global_batch_size={GLOBAL_BATCH_SIZE}\",\n",
        "        f\"data.rollout_batch_size={ROLLOUT_BATCH_SIZE}\",\n",
        "        \"worker.actor.micro_batch_size_per_device_for_update=2\",\n",
        "        \"worker.actor.micro_batch_size_per_device_for_experience=4\",\n",
        "        f\"worker.rollout.n={ROLLOUT_N}\",\n",
        "        f\"worker.rollout.tensor_parallel_size={min(2, N_GPUS)}\",\n",
        "        f\"algorithm.kl_coef={KL_COEF}\",\n",
        "        f\"worker.actor.optim.lr={LEARNING_RATE}\",\n",
        "        f\"trainer.total_epochs={TOTAL_EPOCHS}\",\n",
        "        f\"trainer.save_checkpoint_path={OUTPUT_DIR}\",\n",
        "        \"trainer.save_freq=50\",\n",
        "        'trainer.logger=[\"console\"]',\n",
        "    ]\n",
        "    \n",
        "    process = subprocess.Popen(\n",
        "        cmd,\n",
        "        env=env,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        text=True,\n",
        "        bufsize=1\n",
        "    )\n",
        "    \n",
        "    # Stream output\n",
        "    for line in process.stdout:\n",
        "        print(line, end='')\n",
        "    \n",
        "    process.wait()\n",
        "    print(f\"\\n✓ Training completed with exit code: {process.returncode}\")\n",
        "else:\n",
        "    print(\"Training not started. Set RUN_TRAINING = True to start.\")\n",
        "    print(\"Or run the shell command from Option A in terminal.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Understanding GRPO Reward Function\n",
        "\n",
        "GRPO uses IoU (Intersection over Union) based rewards for grounding tasks:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preview the reward function\n",
        "reward_func_path = FINETUNING_PATH / \"verl\" / \"configs\" / \"reward_func.py\"\n",
        "print(\"GRPO Reward Function (Box IoU):\")\n",
        "print(\"=\" * 50)\n",
        "print(\"\"\"\n",
        "The reward function computes:\n",
        "1. Parse model output for detected boxes\n",
        "2. Match predicted boxes to ground truth by class\n",
        "3. Calculate IoU for each matched pair\n",
        "4. Compute Precision & Recall based on IoU scores\n",
        "5. Return F1 score as final reward\n",
        "\n",
        "Formula:\n",
        "- IoU = intersection_area / union_area\n",
        "- Recall = sum(best_IoU per GT box) / num_GT_boxes\n",
        "- Precision = sum(best_IoU per pred box) / num_pred_boxes\n",
        "- Reward = F1 = 2 * (Precision * Recall) / (Precision + Recall)\n",
        "\n",
        "This encourages the model to:\n",
        "- Detect all objects (high recall)\n",
        "- Avoid false positives (high precision)\n",
        "- Localize objects accurately (high IoU)\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Monitor Training & Load Checkpoints\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for saved checkpoints\n",
        "import glob\n",
        "\n",
        "checkpoint_pattern = str(OUTPUT_DIR / \"**\" / \"*.pt\")\n",
        "checkpoints = glob.glob(checkpoint_pattern, recursive=True)\n",
        "\n",
        "if checkpoints:\n",
        "    print(f\"Found {len(checkpoints)} checkpoints:\")\n",
        "    for ckpt in sorted(checkpoints)[-5:]:  # Show last 5\n",
        "        print(f\"  - {ckpt}\")\n",
        "else:\n",
        "    print(f\"No checkpoints found in {OUTPUT_DIR}\")\n",
        "    print(\"Run training first to generate checkpoints.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and merge GRPO checkpoint to HuggingFace format\n",
        "# This converts the distributed checkpoint to a standard HF model\n",
        "\n",
        "MERGE_CHECKPOINT = False  # Set to True after training\n",
        "\n",
        "if MERGE_CHECKPOINT and checkpoints:\n",
        "    from verl.utils.checkpoint import merge_checkpoint_to_hf\n",
        "    \n",
        "    # Use latest checkpoint\n",
        "    latest_ckpt = sorted(checkpoints)[-1]\n",
        "    output_hf_path = OUTPUT_DIR / \"hf_model\"\n",
        "    \n",
        "    print(f\"Merging checkpoint: {latest_ckpt}\")\n",
        "    print(f\"Output: {output_hf_path}\")\n",
        "    \n",
        "    # Run merge script\n",
        "    merge_cmd = f'''\n",
        "    python {FINETUNING_PATH}/tools/merge_rl_checkpoints_to_hg_version.py \\\\\n",
        "        --checkpoint_path {latest_ckpt} \\\\\n",
        "        --output_path {output_hf_path} \\\\\n",
        "        --model_name {MODEL_NAME}\n",
        "    '''\n",
        "    print(f\"Run this command to merge:\\n{merge_cmd}\")\n",
        "else:\n",
        "    print(\"Set MERGE_CHECKPOINT = True after training to merge checkpoints.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "This notebook sets up GRPO (Group Relative Policy Optimization) training for Rex-Omni on VRSBench data.\n",
        "\n",
        "**Key Components:**\n",
        "1. **TSVRLHFDataset** - Loads TSV data with grounding annotations\n",
        "2. **GroundingTaskFn** - Converts annotations to detection prompts\n",
        "3. **BoxIoU Reward** - Computes F1 score based on detection IoU\n",
        "4. **Ray + vLLM** - Distributed rollout generation\n",
        "5. **FSDP** - Distributed model training\n",
        "\n",
        "**GRPO vs SFT:**\n",
        "| Aspect | SFT | GRPO |\n",
        "|--------|-----|------|\n",
        "| Supervision | Ground truth labels | Reward signals |\n",
        "| Training | Single forward pass | Generate → Evaluate → Update |\n",
        "| Scaling | Single GPU possible | Multi-GPU required |\n",
        "| Memory | ~24GB | ~200GB+ total |\n",
        "\n",
        "**Files Created:**\n",
        "- `data/vrsbench_grpo/` - TSV training data\n",
        "- `configs/grpo_vrsbench.py` - Dataset config\n",
        "- `run_grpo_vrsbench.sh` - Training launch script\n",
        "- `work_dirs/grpo_vrsbench/` - Checkpoints\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

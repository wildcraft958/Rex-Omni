{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Rex-Omni LoRA Fine-tuning on VRSBench Dataset\n",
        "\n",
        "This notebook demonstrates how to fine-tune Rex-Omni with LoRA on the VRSBench grounding dataset.\n",
        "\n",
        "**Requirements:**\n",
        "- GPU with at least 24GB VRAM (A100/RTX 4090 recommended)\n",
        "- Pre-downloaded VRSBench images\n",
        "\n",
        "**Pipeline:**\n",
        "1. Setup environment and imports\n",
        "2. Configuration\n",
        "3. Convert VRSBench to TSV format\n",
        "4. Load model and tokenizer\n",
        "5. Apply LoRA\n",
        "6. Setup dataset and training\n",
        "7. Train and save\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Install Dependencies (Run First!)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install core dependencies with compatible versions\n",
        "\n",
        "print(\"Installing dependencies...\")\n",
        "\n",
        "# Install base packages\n",
        "!uv pip install -q psutil setuptools\n",
        "\n",
        "# Install compatible transformers and peft versions\n",
        "!uv pip install -q transformers==4.57.3\n",
        "!uv pip install -q \"peft==0.18.0\"\n",
        "!uv pip install -q \"bitsandbytes>=0.44.1\"\n",
        "!uv pip install -q torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu124\n",
        "\n",
        "# Install flash attention (requires no-build-isolation)\n",
        "!uv pip install flash-attn==2.7.4.post1 --no-build-isolation\n",
        "\n",
        "# Install other requirements\n",
        "!uv pip install -q accelerate==1.10.1\n",
        "!uv pip install -q mmengine==0.10.7 omegaconf==2.3.0 ujson==5.11.0\n",
        "\n",
        "# Install vision dependencies\n",
        "!uv pip install -q Pillow pandas matplotlib numpy tqdm fastparquet pyarrow\n",
        "!uv pip install -q qwen-vl-utils\n",
        "\n",
        "# Install liger-kernel (required for Qwen2.5-VL training)\n",
        "!uv pip install -q liger-kernel\n",
        "\n",
        "print(\"\\n✓ Dependencies installed successfully!\")\n",
        "\n",
        "print(\"\\nVerifying installations...\")\n",
        "import transformers\n",
        "import peft\n",
        "print(f\"✓ Transformers: {transformers.__version__}\")\n",
        "print(f\"✓ PEFT: {peft.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "using_Modal = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Determine project root (works in Colab and local)\n",
        "if using_Modal:\n",
        "    # Clone repo if in Colab\n",
        "    if not Path('/root/Rex-Omni').exists():\n",
        "        !git clone https://github.com/IDEA-Research/Rex-Omni.git /root/Rex-Omni\n",
        "    PROJECT_ROOT = Path('/root/Rex-Omni')\n",
        "    os.chdir(PROJECT_ROOT)\n",
        "else:\n",
        "    # Local environment - find project root\n",
        "    PROJECT_ROOT = Path.cwd()\n",
        "    while PROJECT_ROOT.name != 'Rex-Omni' and PROJECT_ROOT.parent != PROJECT_ROOT:\n",
        "        PROJECT_ROOT = PROJECT_ROOT.parent\n",
        "    if PROJECT_ROOT.name != 'Rex-Omni':\n",
        "        PROJECT_ROOT = Path.cwd()\n",
        "    os.chdir(PROJECT_ROOT)\n",
        "\n",
        "# Add finetuning to path\n",
        "FINETUNING_PATH = PROJECT_ROOT / 'finetuning'\n",
        "for p in [str(PROJECT_ROOT), str(FINETUNING_PATH)]:\n",
        "    if p not in sys.path:\n",
        "        sys.path.insert(0, p)\n",
        "\n",
        "print(f\"✓ Project root: {PROJECT_ROOT}\")\n",
        "print(f\"✓ Finetuning path: {FINETUNING_PATH}\")\n",
        "print(f\"✓ Working directory: {os.getcwd()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core imports\n",
        "import json\n",
        "import base64\n",
        "import io\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import transformers\n",
        "from PIL import Image\n",
        "\n",
        "# Liger kernel - MUST be imported before model loading\n",
        "from liger_kernel.transformers import apply_liger_kernel_to_qwen2_5_vl\n",
        "\n",
        "# Transformers components\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoProcessor,\n",
        "    Qwen2_5_VLForConditionalGeneration,\n",
        "    Trainer,\n",
        ")\n",
        "from engine.argument import TrainingArguments\n",
        "\n",
        "# PEFT for LoRA\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "# Finetuning components\n",
        "from engine.argument import DataArguments\n",
        "from dataset.tsv_dataset import GroundingTSVDataset\n",
        "from dataset.collator import DataCollatorForSupervisedDataset\n",
        "from dataset.task_fns import GroundingTaskFn\n",
        "from dataset.task_fns.task_prompts.grounding_task import GROUNDING_SINGLE_REGION_STAGE_XYXY\n",
        "\n",
        "print(f\"✓ PyTorch: {torch.__version__}\")\n",
        "print(f\"✓ Transformers: {transformers.__version__}\")\n",
        "print(f\"✓ CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"✓ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"✓ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "from dataset.json_dataset import GroundingJsonDataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== MODEL ====================\n",
        "MODEL_NAME = \"IDEA-Research/Rex-Omni\"\n",
        "\n",
        "# ==================== DATA ====================\n",
        "VRSBENCH_PARQUET = PROJECT_ROOT / \"vrsbench_val_data.parquet\"\n",
        "VRSBENCH_IMAGES_DIR = PROJECT_ROOT / \"Images_validation\" / \"Images_val\"  # Update this path!\n",
        "TSV_OUTPUT_DIR = PROJECT_ROOT / \"data\" / \"vrsbench_finetune\"\n",
        "NUM_SAMPLES = 1000  # Number of samples to use for finetuning\n",
        "\n",
        "# Sequence length (model will auto-detect pixel values)\n",
        "MAX_LENGTH = 4096\n",
        "\n",
        "# ==================== LORA ====================\n",
        "LORA_CONFIG = LoraConfig(\n",
        "    r=64,\n",
        "    lora_alpha=128,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        ")\n",
        "\n",
        "# ==================== TRAINING ====================\n",
        "OUTPUT_DIR = PROJECT_ROOT / \"outputs\" / \"rex-omni-lora-vrsbench\"\n",
        "BATCH_SIZE = 1\n",
        "GRADIENT_ACCUMULATION_STEPS = 8\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_EPOCHS = 3\n",
        "WARMUP_RATIO = 0.03\n",
        "SAVE_STEPS = 100\n",
        "LOGGING_STEPS = 10\n",
        "\n",
        "print(\"Configuration:\")\n",
        "print(f\"  Model: {MODEL_NAME}\")\n",
        "print(f\"  Samples: {NUM_SAMPLES}\")\n",
        "print(f\"  LoRA rank: {LORA_CONFIG.r}\")\n",
        "print(f\"  Batch size: {BATCH_SIZE} x {GRADIENT_ACCUMULATION_STEPS} = {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
        "print(f\"  Output: {OUTPUT_DIR}\")\n",
        "print(\"\\n  Note: Image pixel values will be auto-detected from model config\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Convert VRSBench to TSV Format\n",
        "\n",
        "The finetuning code expects data in TSV format:\n",
        "- `train.images.tsv`: `{byte_offset}\\t{base64_image}\\n`\n",
        "- `train.annotations.tsv`: `{img_byte_offset}\\t{annotation_json}\\n`\n",
        "- `train.annotations.tsv.lineidx`: Byte offsets for annotation file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_vrsbench_to_json(\n",
        "    parquet_path: Path,\n",
        "    images_dir: Path,\n",
        "    output_dir: Path,\n",
        "    num_samples: int = 1000,\n",
        "    project_root: Path = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Convert VRSBench parquet to a simple JSON dataset file.\n",
        "    Output format: List[Dict] where each dict has 'image_path', 'boxes', 'labels'.\n",
        "    \"\"\"\n",
        "    import json\n",
        "    import shutil\n",
        "    \n",
        "    output_dir = Path(output_dir)\n",
        "    if output_dir.exists():\n",
        "        shutil.rmtree(output_dir)\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    json_file = output_dir / \"train.json\"\n",
        "    \n",
        "    print(f\"Loading {parquet_path}...\")\n",
        "    df = pd.read_parquet(parquet_path)\n",
        "    \n",
        "    df_sample = df.head(num_samples).copy()\n",
        "    print(f\"  Processing {len(df_sample)} samples\")\n",
        "    \n",
        "    dataset_list = []\n",
        "    skipped = 0\n",
        "    \n",
        "    for idx, row in tqdm(df_sample.iterrows(), total=len(df_sample), desc=\"Converting\"):\n",
        "        try:\n",
        "            # 1. Resolve Image Path\n",
        "            image_path = row.get('image_path', '')\n",
        "            if not image_path:\n",
        "                skipped += 1\n",
        "                continue\n",
        "                \n",
        "            img_path = None\n",
        "            candidates = [\n",
        "                Path(image_path),\n",
        "                images_dir / Path(image_path).name,\n",
        "                project_root / image_path.lstrip('./') if project_root else None,\n",
        "            ]\n",
        "            for candidate in candidates:\n",
        "                if candidate and candidate.exists():\n",
        "                    img_path = candidate\n",
        "                    break\n",
        "            \n",
        "            if img_path is None:\n",
        "                skipped += 1\n",
        "                continue\n",
        "                \n",
        "            # 2. Load Image (to get size for denormalization)\n",
        "            try:\n",
        "                pil_img = Image.open(img_path)\n",
        "                width, height = pil_img.size\n",
        "            except Exception:\n",
        "                skipped += 1\n",
        "                continue\n",
        "                \n",
        "            # 3. Parse Annotations\n",
        "            objects_data = row.get('objects')\n",
        "            if isinstance(objects_data, str):\n",
        "                objects_list = json.loads(objects_data)\n",
        "            else:\n",
        "                objects_list = objects_data\n",
        "                \n",
        "            if not objects_list:\n",
        "                skipped += 1\n",
        "                continue\n",
        "                \n",
        "            boxes = []\n",
        "            labels = []\n",
        "            \n",
        "            for obj in objects_list:\n",
        "                bbox_norm = obj.get('obj_coord')\n",
        "                if not bbox_norm or len(bbox_norm) != 4:\n",
        "                    continue\n",
        "                    \n",
        "                # Denormalize [0-1] -> [pixel]\n",
        "                x0, y0, x1, y1 = bbox_norm\n",
        "                boxes.append([\n",
        "                    x0 * width, \n",
        "                    y0 * height, \n",
        "                    x1 * width, \n",
        "                    y1 * height\n",
        "                ])\n",
        "                \n",
        "                phrase = obj.get('referring_sentence', '') or obj.get('obj_cls', '')\n",
        "                labels.append(str(phrase))\n",
        "                \n",
        "            if not boxes:\n",
        "                skipped += 1\n",
        "                continue\n",
        "                \n",
        "            dataset_list.append({\n",
        "                \"image_path\": str(img_path.absolute()),\n",
        "                \"boxes\": boxes,\n",
        "                \"labels\": labels\n",
        "            })\n",
        "            \n",
        "        except Exception as e:\n",
        "            skipped += 1\n",
        "            continue\n",
        "            \n",
        "    print(f\"\\nSaving {len(dataset_list)} samples to {json_file}...\")\n",
        "    with open(json_file, 'w') as f:\n",
        "        json.dump(dataset_list, f, indent=2)\n",
        "        \n",
        "    print(f\"Done! Skipped {skipped} items.\")\n",
        "    return len(dataset_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to JSON\n",
        "if TSV_OUTPUT_DIR.exists():\n",
        "    import shutil\n",
        "    shutil.rmtree(TSV_OUTPUT_DIR)\n",
        "\n",
        "print(\"Converting VRSBench to JSON format...\")\n",
        "num_converted = convert_vrsbench_to_json(\n",
        "    parquet_path=VRSBENCH_PARQUET,\n",
        "    images_dir=VRSBENCH_IMAGES_DIR,\n",
        "    output_dir=TSV_OUTPUT_DIR,\n",
        "    num_samples=NUM_SAMPLES,\n",
        "    project_root=PROJECT_ROOT\n",
        ")\n",
        "print(f\"\\n✓ Ready for training with {num_converted} samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load Model, Tokenizer, and Processor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Loading tokenizer and processor...\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    model_max_length=MAX_LENGTH,\n",
        "    padding_side=\"right\",\n",
        "    use_fast=False,\n",
        ")\n",
        "print(f\"✓ Tokenizer loaded (vocab size: {len(tokenizer)})\")\n",
        "\n",
        "# Load processor (for image processing)\n",
        "# Uses model's default min/max_pixels from config - no need to specify manually\n",
        "processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Print what defaults the model uses\n",
        "img_proc = processor.image_processor\n",
        "print(f\"✓ Processor loaded\")\n",
        "print(f\"  min_pixels: {getattr(img_proc, 'min_pixels', 'default')}\")\n",
        "print(f\"  max_pixels: {getattr(img_proc, 'max_pixels', 'default')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Loading model...\")\n",
        "\n",
        "# CRITICAL: Apply liger kernel BEFORE loading model (as in train.py line 114)\n",
        "apply_liger_kernel_to_qwen2_5_vl(fused_linear_cross_entropy=False)\n",
        "print(\"✓ Liger kernel applied\")\n",
        "\n",
        "# Load model with correct class (Qwen2_5_VLForConditionalGeneration, NOT AutoModelForCausalLM)\n",
        "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.bfloat16,  # FIXED: was 'dtype', should be 'torch_dtype'\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model.config.use_cache = False  # Disable KV cache for training\n",
        "\n",
        "print(f\"✓ Model loaded\")\n",
        "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B\")\n",
        "print(f\"  Device: {next(model.parameters()).device}\")\n",
        "print(f\"  Dtype: {next(model.parameters()).dtype}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Apply LoRA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Applying LoRA...\")\n",
        "\n",
        "# Enable gradient checkpointing for memory efficiency\n",
        "if hasattr(model, \"enable_input_require_grads\"):\n",
        "    model.enable_input_require_grads()\n",
        "else:\n",
        "    def make_inputs_require_grad(module, input, output):\n",
        "        output.requires_grad_(True)\n",
        "    model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n",
        "\n",
        "# Apply LoRA\n",
        "model = get_peft_model(model, LORA_CONFIG)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Verify trainable parameters\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total = sum(p.numel() for p in model.parameters())\n",
        "print(f\"\\n✓ LoRA applied\")\n",
        "print(f\"  Trainable: {trainable / 1e6:.2f}M ({100 * trainable / total:.3f}%)\")\n",
        "\n",
        "if trainable == 0:\n",
        "    raise RuntimeError(\"❌ No trainable parameters! LoRA not applied correctly.\")\n",
        "\n",
        "# List some trainable parameters for verification\n",
        "print(\"\\n  Sample trainable params:\")\n",
        "trainable_names = [n for n, p in model.named_parameters() if p.requires_grad]\n",
        "for name in trainable_names[:5]:\n",
        "    print(f\"    - {name}\")\n",
        "print(f\"    ... and {len(trainable_names) - 5} more\")\n",
        "\n",
        "# Ensure model is in training mode\n",
        "model.train()\n",
        "\n",
        "# Note: The warning \"None of the inputs have requires_grad=True\" during training\n",
        "# is NORMAL for the frozen vision encoder parts. As long as loss is decreasing,\n",
        "# LoRA is training correctly.\n",
        "print(\"\\n✓ Model ready for training\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Setup Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Setting up dataset...\")\n",
        "\n",
        "data_args = DataArguments()\n",
        "data_args.image_processor = processor.image_processor\n",
        "data_args.model_type = \"qwen2.5vl\"\n",
        "\n",
        "# Auto-detect pixel limits\n",
        "img_proc = processor.image_processor\n",
        "min_pixels = getattr(img_proc, 'min_pixels', 4 * 28 * 28)\n",
        "max_pixels = getattr(img_proc, 'max_pixels', 16384 * 28 * 28)\n",
        "\n",
        "print(f\"✓ DataArguments created\")\n",
        "print(f\"  min_pixels: {min_pixels}\")\n",
        "print(f\"  max_pixels: {max_pixels}\")\n",
        "\n",
        "# JSON file path\n",
        "json_file = str(TSV_OUTPUT_DIR / \"train.json\")\n",
        "\n",
        "if not Path(json_file).exists():\n",
        "    raise FileNotFoundError(f\"Dataset file not found: {json_file}\\nRun conversion first!\")\n",
        "\n",
        "task_fn_config = dict(\n",
        "    type=GroundingTaskFn,\n",
        "    task_prompts=GROUNDING_SINGLE_REGION_STAGE_XYXY,\n",
        "    image_min_pixels=min_pixels,\n",
        "    image_max_pixels=max_pixels,\n",
        ")\n",
        "\n",
        "# Instantiate JSON Dataset\n",
        "from dataset.json_dataset import GroundingJsonDataset\n",
        "\n",
        "train_dataset = GroundingJsonDataset(\n",
        "    json_file=json_file,\n",
        "    tokenizer=tokenizer,\n",
        "    data_args=data_args,\n",
        "    image_min_pixels=min_pixels,\n",
        "    image_max_pixels=max_pixels,\n",
        "    task_fn=task_fn_config,\n",
        "    system_message=\"You are a helpful assistant.\",\n",
        "    ori_box_format=\"xyxy\",\n",
        "    dataset_name=\"vrsbench_grounding\",\n",
        "    max_length=MAX_LENGTH,\n",
        ")\n",
        "\n",
        "print(f\"✓ Dataset created: {len(train_dataset)} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validate dataset before training - test multiple samples to catch TSV errors early\n",
        "print(\"Validating dataset (testing first 50 samples)...\")\n",
        "print(\"This catches TSV format errors before training starts.\\n\")\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "valid_samples = 0\n",
        "error_samples = []\n",
        "\n",
        "# Test first 50 samples (or all if less)\n",
        "test_count = min(50, len(train_dataset))\n",
        "\n",
        "for i in tqdm(range(test_count), desc=\"Validating\"):\n",
        "    try:\n",
        "        sample = train_dataset[i]\n",
        "        valid_samples += 1\n",
        "    except Exception as e:\n",
        "        error_samples.append((i, str(e)))\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"Validation Results:\")\n",
        "print(f\"  ✓ Valid samples: {valid_samples}/{test_count}\")\n",
        "print(f\"  ✗ Error samples: {len(error_samples)}/{test_count}\")\n",
        "\n",
        "if error_samples:\n",
        "    print(f\"\\nFirst 5 errors:\")\n",
        "    for idx, err in error_samples[:5]:\n",
        "        print(f\"  Sample {idx}: {err[:80]}...\")\n",
        "    \n",
        "    error_rate = len(error_samples) / test_count\n",
        "    if error_rate > 0.1:  # More than 10% errors\n",
        "        print(f\"\\n❌ ERROR RATE TOO HIGH ({error_rate*100:.1f}%)\")\n",
        "        print(\"   This indicates corrupted TSV files.\")\n",
        "        print(\"   Please re-run Cell 10 (conversion) and Cell 11 to regenerate data.\")\n",
        "        raise RuntimeError(f\"Too many corrupt samples: {len(error_samples)}/{test_count}\")\n",
        "    else:\n",
        "        print(f\"\\n⚠️  Some samples have errors but error rate is acceptable ({error_rate*100:.1f}%)\")\n",
        "        print(\"   Training will skip these samples.\")\n",
        "else:\n",
        "    print(\"\\n✓ All tested samples are valid!\")\n",
        "\n",
        "# Show sample structure\n",
        "print(\"\\nSample data structure:\")\n",
        "sample = train_dataset[0]\n",
        "print(f\"  Keys: {list(sample.keys())}\")\n",
        "print(f\"  input_ids shape: {sample['input_ids'].shape}\")\n",
        "print(f\"  labels shape: {sample['labels'].shape}\")\n",
        "if 'pixel_values' in sample:\n",
        "    pv = sample['pixel_values']\n",
        "    if isinstance(pv, list):\n",
        "        print(f\"  pixel_values: {len(pv)} tensors\")\n",
        "    else:\n",
        "        print(f\"  pixel_values shape: {pv.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create data collator (only takes tokenizer, see collator.py line 36)\n",
        "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
        "\n",
        "# Test collator\n",
        "print(\"Testing data collator...\")\n",
        "batch = data_collator([train_dataset[0]])\n",
        "print(f\"✓ Batch created\")\n",
        "print(f\"  Keys: {list(batch.keys())}\")\n",
        "print(f\"  input_ids: {batch['input_ids'].shape}\")\n",
        "print(f\"  labels: {batch['labels'].shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create output directory\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=str(OUTPUT_DIR),\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    logging_steps=LOGGING_STEPS,\n",
        "    save_steps=SAVE_STEPS,\n",
        "    save_total_limit=3,\n",
        "    bf16=True,\n",
        "    gradient_checkpointing=True,\n",
        "    dataloader_num_workers=4,\n",
        "    remove_unused_columns=False,  # Important for custom datasets\n",
        "    report_to=\"none\",  # Disable wandb/tensorboard\n",
        "    optim=\"adamw_torch\",\n",
        ")\n",
        "\n",
        "print(f\"Training configuration:\")\n",
        "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"  Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
        "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"  Output: {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    data_collator=data_collator,\n",
        "    processing_class=tokenizer,\n",
        ")\n",
        "\n",
        "print(\"✓ Trainer created\")\n",
        "print(f\"  Dataset size: {len(train_dataset)}\")\n",
        "print(f\"  Steps per epoch: {len(train_dataset) // (BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start training\n",
        "print(\"Starting training...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"✓ Training complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Save Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save final model\n",
        "final_output = OUTPUT_DIR / \"final\"\n",
        "final_output.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Saving model to {final_output}...\")\n",
        "\n",
        "# Save LoRA adapter\n",
        "model.save_pretrained(str(final_output))\n",
        "tokenizer.save_pretrained(str(final_output))\n",
        "\n",
        "# Save image processor\n",
        "processor.image_processor.save_pretrained(str(final_output))\n",
        "\n",
        "# Copy chat template if available\n",
        "import shutil\n",
        "try:\n",
        "    from huggingface_hub import hf_hub_download\n",
        "    chat_template_src = hf_hub_download(repo_id=MODEL_NAME, filename=\"chat_template.json\")\n",
        "    shutil.copy(chat_template_src, str(final_output / \"chat_template.json\"))  # FIXED: convert Path to str\n",
        "    print(\"  ✓ Chat template copied\")\n",
        "except Exception:\n",
        "    print(\"  Note: chat_template.json not found, skipping\")\n",
        "\n",
        "print(f\"\\n✓ Model saved to: {final_output}\")\n",
        "print(f\"\\nTo load the model later:\")\n",
        "print(f\"  from peft import PeftModel\")\n",
        "print(f\"  model = Qwen2_5_VLForConditionalGeneration.from_pretrained('{MODEL_NAME}')\")\n",
        "print(f\"  model = PeftModel.from_pretrained(model, '{final_output}')\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. (Optional) Test Inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick inference test - using CORRECT pattern from RexOmniWrapper\n",
        "print(\"Testing inference...\")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Load a test image\n",
        "test_images = list(VRSBENCH_IMAGES_DIR.glob(\"*.png\"))[:1] if VRSBENCH_IMAGES_DIR.exists() else []\n",
        "if test_images:\n",
        "    test_img_path = test_images[0]\n",
        "    test_img = Image.open(test_img_path).convert('RGB')\n",
        "    print(f\"Test image: {test_img_path.name} ({test_img.size})\")\n",
        "    \n",
        "    # Format message for Qwen2.5-VL\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": test_img},\n",
        "                {\"type\": \"text\", \"text\": \"Detect all objects in this image.\"}\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    # CORRECT PATTERN (from RexOmniWrapper._generate_transformers):\n",
        "    # Step 1: Apply chat template\n",
        "    text = processor.apply_chat_template(\n",
        "        messages, \n",
        "        tokenize=False, \n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    \n",
        "    # Step 2: Process with processor() - returns 2D pixel_values (CORRECT for Qwen2.5-VL!)\n",
        "    # The model expects flattened patches, NOT 4D tensors\n",
        "    inputs = processor(\n",
        "        text=[text],\n",
        "        images=[test_img],\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    \n",
        "    # Move to device\n",
        "    device = next(model.parameters()).device\n",
        "    inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
        "    \n",
        "    print(f\"  input_ids shape: {inputs['input_ids'].shape}\")\n",
        "    print(f\"  pixel_values shape: {inputs['pixel_values'].shape}\")\n",
        "    print(f\"  image_grid_thw: {inputs.get('image_grid_thw', 'N/A')}\")\n",
        "    \n",
        "    # Note: 2D pixel_values is CORRECT for Qwen2.5-VL (flattened patches)\n",
        "    # The model handles this format internally\n",
        "    \n",
        "    # Generate\n",
        "    print(\"\\nGenerating...\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=512,\n",
        "            do_sample=False,\n",
        "            pad_token_id=processor.tokenizer.eos_token_id,\n",
        "        )\n",
        "    \n",
        "    # Decode response - trim input tokens\n",
        "    generated_ids = outputs[0][inputs['input_ids'].shape[1]:]\n",
        "    response = processor.decode(generated_ids, skip_special_tokens=True)\n",
        "    \n",
        "    print(f\"\\n✓ Inference successful!\")\n",
        "    print(f\"\\nResponse:\\n{response}\")\n",
        "else:\n",
        "    print(\"No test images found - skipping inference test\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "This notebook fine-tuned Rex-Omni with LoRA on VRSBench grounding data.\n",
        "\n",
        "**Key components aligned with finetuning codebase:**\n",
        "1. **Model**: `Qwen2_5_VLForConditionalGeneration` (from train.py line 115)\n",
        "2. **Liger Kernel**: Applied before model loading (train.py line 114)\n",
        "3. **TSV Format**: Three files matching `convert_json_data_to_tsv.py` format\n",
        "4. **DataArguments**: min_pixels, max_pixels + dynamic model_type, image_processor (train.py lines 122-125)\n",
        "5. **Task Function**: `GroundingTaskFn` with `GROUNDING_SINGLE_REGION_STAGE_XYXY` prompts (sft.py config)\n",
        "6. **Data Collator**: `DataCollatorForSupervisedDataset(tokenizer=tokenizer)` (collator.py line 36)\n",
        "\n",
        "**Output:**\n",
        "- LoRA adapter saved to `outputs/rex-omni-lora-vrsbench/final/`\n",
        "- Load with `PeftModel.from_pretrained()`\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

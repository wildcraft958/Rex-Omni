{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-tuning Rex-Omni with LoRA on VRSBench Dataset\n",
        "\n",
        "This notebook fine-tunes the Rex-Omni model using LoRA (Low-Rank Adaptation) on 1000 samples from the VRSBench validation dataset.\n",
        "\n",
        "## Why This Approach Works\n",
        "\n",
        "**Rex-Omni uses the standard Qwen2.5-VL architecture from Hugging Face**, so LoRA can be applied directly:\n",
        "- ✅ Model architecture is standard (no custom modifications)\n",
        "- ✅ LoRA works with standard attention/MLP layers\n",
        "- ✅ Simpler and more flexible than using the full finetuning codebase\n",
        "- ✅ Directly loads from `IDEA-Research/Rex-Omni` on Hugging Face\n",
        "\n",
        "**Note**: The \"architecture changes\" people mention refer to training infrastructure (data processing, custom trainers), not the model architecture itself.\n",
        "\n",
        "## Overview\n",
        "1. Load and explore VRSBench parquet data\n",
        "2. Convert data to TSV format required for training\n",
        "3. Set up LoRA configuration\n",
        "4. Fine-tune the model with LoRA\n",
        "5. Save and evaluate the fine-tuned model\n",
        "\n",
        "## Best Practices & Troubleshooting\n",
        "- **CUDA Errors**: If you see `indexSelectLargeIndex` or device-side asserts, it usually means `image_grid_thw` mismatch. We have added safety checks in the dataset and wrapper to catch this early.\n",
        "- **Restart Runtime**: If a CUDA error occurs, you **must** restart the runtime/kernel to recover.\n",
        "- **Canonical Flow**: We strictly use `qwen_vl_utils.process_vision_info` to ensure alignment between pixel values and grid shapes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup for Google Colab\n",
        "\n",
        "### 1.1 Clone Repository and Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import importlib\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "print(f\"Detected Google Colab runtime: {IN_COLAB}\")\n",
        "\n",
        "if not IN_COLAB:\n",
        "    print(\"Skipping Colab bootstrap because this runtime is not Google Colab.\")\n",
        "    print(\"Ensure dependencies are installed locally via `pip install -e .` from the project root.\")\n",
        "else:\n",
        "    repo_path = Path(\"/content/Rex-Omni\")\n",
        "\n",
        "    def run_cmd(cmd: str, check: bool = True):\n",
        "        print(f\"\\n$ {cmd}\")\n",
        "        completed = subprocess.run(cmd, shell=True)\n",
        "        if check and completed.returncode != 0:\n",
        "            raise RuntimeError(f\"Command failed with exit code {completed.returncode}: {cmd}\")\n",
        "\n",
        "    if not repo_path.exists():\n",
        "        run_cmd(\"git clone https://github.com/IDEA-Research/Rex-Omni.git /content/Rex-Omni\")\n",
        "    else:\n",
        "        print(\"Repository already exists at /content/Rex-Omni; skipping clone.\")\n",
        "\n",
        "    os.chdir(repo_path)\n",
        "    print(f\"Working directory set to: {os.getcwd()}\")\n",
        "\n",
        "    print(\"Step 1: Installing compatible PyTorch versions...\")\n",
        "    run_cmd(\"pip install -q torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu124 --force-reinstall\")\n",
        "\n",
        "    print(\"Step 2: Installing compatible numpy...\")\n",
        "    run_cmd('pip install -q \"numpy<2.0\" --force-reinstall')\n",
        "\n",
        "    print(\"Step 3: Installing transformers with modeling_layers support...\")\n",
        "    run_cmd('pip install -q \"transformers>=4.44.0,<5.0.0\" --upgrade')\n",
        "\n",
        "    print(\"Step 4: Verifying transformers.modeling_layers...\")\n",
        "    if 'transformers' in sys.modules:\n",
        "        importlib.reload(sys.modules['transformers'])\n",
        "\n",
        "    def check_modeling_layers():\n",
        "        try:\n",
        "            from transformers.modeling_utils import modeling_layers  # noqa: F401\n",
        "            print(\"\\u2713 transformers.modeling_layers found via modeling_utils!\")\n",
        "            return True\n",
        "        except ImportError:\n",
        "            try:\n",
        "                from transformers import modeling_layers  # type: ignore  # noqa: F401\n",
        "                print(\"\\u2713 transformers.modeling_layers found!\")\n",
        "                return True\n",
        "            except ImportError:\n",
        "                return False\n",
        "\n",
        "    if not check_modeling_layers():\n",
        "        print(\"\\u2717 modeling_layers not found. Installing from source...\")\n",
        "        run_cmd(\"pip uninstall -y transformers -q\")\n",
        "        run_cmd(\"pip install -q git+https://github.com/huggingface/transformers.git --no-deps\")\n",
        "        run_cmd('pip install -q \"transformers[torch]\" --upgrade')\n",
        "        if not check_modeling_layers():\n",
        "            print(\"\\u26a0 WARNING: modeling_layers still not found. You may need to use peft==0.13.0.\")\n",
        "\n",
        "    print(\"Step 5: Installing peft...\")\n",
        "    try:\n",
        "        run_cmd('pip install -q \"peft>=0.18.0\" --upgrade')\n",
        "        import peft\n",
        "        print(f\"\\u2713 peft {peft.__version__} installed successfully!\")\n",
        "    except Exception as exc:  # noqa: BLE001\n",
        "        print(f\"\\u26a0 peft >=0.18.0 installation failed: {exc}\")\n",
        "        print(\"Trying older compatible version...\")\n",
        "        run_cmd('pip install -q \"peft==0.13.0\" --upgrade')\n",
        "\n",
        "    print(\"Step 6: Installing other dependencies...\")\n",
        "    run_cmd(\"pip install -q accelerate datasets pandas pyarrow pillow\")\n",
        "\n",
        "    print(\"Trying to install flash-attn (safe to fail)...\")\n",
        "    flash_status = subprocess.run(\"pip install -q flash-attn --no-build-isolation\", shell=True)\n",
        "    if flash_status.returncode == 0:\n",
        "        print(\"\\u2713 flash-attn installed\")\n",
        "    else:\n",
        "        print(\"\\u26a0 flash-attn installation failed. Continuing without it...\")\n",
        "\n",
        "    print(\"Step 7: Installing Rex-Omni package (no deps)...\")\n",
        "    run_cmd(\"pip install -v -e . --no-deps\")\n",
        "\n",
        "    print(\"Step 8: Installing finetuning dependencies (no deps)...\")\n",
        "    run_cmd(\"cd finetuning && pip install -v -e . --no-deps\")\n",
        "\n",
        "    print(\"Step 9: Installing Rex-Omni extra dependencies...\")\n",
        "    run_cmd(\"pip install -q qwen_vl_utils==0.0.14 vllm==0.8.2 gradio==4.44.1 gradio_image_prompter==0.1.0 pydantic==2.10.6 pycocotools==2.0.10 shapely==2.1.2 gradio_bbox_annotator==0.1.1\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"FINAL VERIFICATION\")\n",
        "    print(\"=\" * 50)\n",
        "    import torch\n",
        "    import transformers\n",
        "    try:\n",
        "        import peft  # noqa: F401\n",
        "        print(f\"\\u2713 torch: {torch.__version__}\")\n",
        "        print(f\"\\u2713 transformers: {transformers.__version__}\")\n",
        "        print(f\"\\u2713 peft: {peft.__version__}\")\n",
        "        if not check_modeling_layers():\n",
        "            print(\"\\u26a0 transformers.modeling_layers still unavailable; peft >=0.18 may fail.\")\n",
        "        else:\n",
        "            print(\"\\u2713 transformers.modeling_layers available\")\n",
        "        print(\"\\n\\u2713 Installation complete! Continue with the notebook.\")\n",
        "    except Exception as exc:  # noqa: BLE001\n",
        "        print(f\"\\u2717 Verification failed: {exc}\")\n",
        "        print(\"Please re-run the troubleshooting cell if needed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1.1 Post-Installation Verification\n",
        "\n",
        "**After running Cell 2, run this cell to verify everything is working:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Post-installation verification and fix\n",
        "print(\"=\"*60)\n",
        "print(\"POST-INSTALLATION VERIFICATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Check versions\n",
        "import torch\n",
        "import transformers\n",
        "import sys\n",
        "\n",
        "print(f\"\\nCurrent versions:\")\n",
        "print(f\"  torch: {torch.__version__}\")\n",
        "print(f\"  transformers: {transformers.__version__}\")\n",
        "\n",
        "# Check for modeling_layers - it might be in different locations\n",
        "print(f\"\\nChecking for modeling_layers...\")\n",
        "modeling_layers_found = False\n",
        "modeling_layers_location = None\n",
        "\n",
        "# Try different import paths\n",
        "try:\n",
        "    from transformers import modeling_layers\n",
        "    modeling_layers_found = True\n",
        "    modeling_layers_location = \"transformers.modeling_layers\"\n",
        "    print(\"✓ Found: transformers.modeling_layers\")\n",
        "except ImportError:\n",
        "    try:\n",
        "        from transformers.modeling_utils import modeling_layers\n",
        "        modeling_layers_found = True\n",
        "        modeling_layers_location = \"transformers.modeling_utils.modeling_layers\"\n",
        "        print(\"✓ Found: transformers.modeling_utils.modeling_layers\")\n",
        "    except ImportError:\n",
        "        try:\n",
        "            # Check if it exists as an attribute\n",
        "            if hasattr(transformers, 'modeling_layers'):\n",
        "                modeling_layers_found = True\n",
        "                modeling_layers_location = \"transformers.modeling_layers (attribute)\"\n",
        "                print(\"✓ Found: transformers.modeling_layers (as attribute)\")\n",
        "            elif hasattr(transformers.modeling_utils, 'modeling_layers'):\n",
        "                modeling_layers_found = True\n",
        "                modeling_layers_location = \"transformers.modeling_utils.modeling_layers (attribute)\"\n",
        "                print(\"✓ Found: transformers.modeling_utils.modeling_layers (as attribute)\")\n",
        "            else:\n",
        "                print(\"✗ modeling_layers not found in standard locations\")\n",
        "        except:\n",
        "            print(\"✗ modeling_layers not found\")\n",
        "\n",
        "# Try importing peft to see if it works\n",
        "print(f\"\\nChecking peft...\")\n",
        "try:\n",
        "    import peft\n",
        "    print(f\"✓ peft {peft.__version__} imported successfully\")\n",
        "    \n",
        "    # Try to use peft to see if modeling_layers is actually needed\n",
        "    try:\n",
        "        from peft import LoraConfig\n",
        "        print(\"✓ LoraConfig imported successfully\")\n",
        "        peft_works = True\n",
        "    except Exception as e:\n",
        "        print(f\"✗ LoraConfig import failed: {e}\")\n",
        "        peft_works = False\n",
        "except Exception as e:\n",
        "    print(f\"✗ peft import failed: {e}\")\n",
        "    peft_works = False\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "if modeling_layers_found:\n",
        "    print(f\"✓ modeling_layers found at: {modeling_layers_location}\")\n",
        "else:\n",
        "    print(\"⚠ modeling_layers not found, but this might be okay\")\n",
        "    print(\"  Some transformers versions don't expose it directly\")\n",
        "\n",
        "if peft_works:\n",
        "    print(\"✓ peft is working correctly\")\n",
        "    print(\"\\n✅ Everything looks good! You can proceed with the notebook.\")\n",
        "else:\n",
        "    print(\"✗ peft has issues\")\n",
        "    print(\"\\n⚠ You may need to:\")\n",
        "    print(\"  1. Restart the runtime (Runtime → Restart runtime)\")\n",
        "    print(\"  2. Run Cell 2 again\")\n",
        "    print(\"  3. Or use peft 0.13.0 instead of 0.18.0+\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1.0 Quick Fix for Current Error\n",
        "\n",
        "**If you're seeing the `modeling_layers` error right now, run the cell below first:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# QUICK FIX: Run this cell if you're getting version conflicts or modeling_layers errors\n",
        "# This will fix PyTorch/transformers/peft compatibility issues\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"QUICK FIX: Resolving version conflicts...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Step 1: Fix PyTorch version conflicts\n",
        "print(\"\\n1. Fixing PyTorch versions...\")\n",
        "!pip uninstall -y torch torchvision torchaudio -q\n",
        "!pip install -q torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu124\n",
        "\n",
        "# Step 2: Fix numpy\n",
        "print(\"\\n2. Fixing numpy version...\")\n",
        "!pip install -q \"numpy<2.0\" --force-reinstall\n",
        "\n",
        "# Step 3: Reinstall transformers\n",
        "print(\"\\n3. Reinstalling transformers...\")\n",
        "!pip uninstall -y transformers -q\n",
        "!pip install -q \"transformers>=4.44.0,<5.0.0\" --upgrade\n",
        "\n",
        "# Step 4: Verify modeling_layers\n",
        "print(\"\\n4. Verifying transformers.modeling_layers...\")\n",
        "import importlib\n",
        "import sys\n",
        "if 'transformers' in sys.modules:\n",
        "    importlib.reload(sys.modules['transformers'])\n",
        "\n",
        "try:\n",
        "    from transformers import modeling_layers\n",
        "    print(\"✓ transformers.modeling_layers found!\")\n",
        "except ImportError:\n",
        "    try:\n",
        "        from transformers.modeling_utils import modeling_layers\n",
        "        print(\"✓ transformers.modeling_layers found (via modeling_utils)!\")\n",
        "    except ImportError:\n",
        "        print(\"⚠ modeling_layers not found. Installing from source...\")\n",
        "        !pip uninstall -y transformers -q\n",
        "        !pip install -q git+https://github.com/huggingface/transformers.git --no-deps\n",
        "        !pip install -q \"transformers[torch]\" --upgrade\n",
        "\n",
        "# Step 5: Reinstall peft\n",
        "print(\"\\n5. Reinstalling peft...\")\n",
        "try:\n",
        "    !pip install -q \"peft>=0.18.0\" --upgrade --force-reinstall\n",
        "    import peft\n",
        "    print(f\"✓ peft {peft.__version__} installed!\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠ peft 0.18.0+ failed: {e}\")\n",
        "    print(\"Installing older compatible version...\")\n",
        "    !pip install -q \"peft==0.13.0\" --upgrade --force-reinstall\n",
        "\n",
        "# Final verification\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"VERIFICATION\")\n",
        "print(\"=\"*60)\n",
        "try:\n",
        "    import torch\n",
        "    import transformers\n",
        "    import peft\n",
        "    print(f\"✓ torch: {torch.__version__}\")\n",
        "    print(f\"✓ transformers: {transformers.__version__}\")\n",
        "    print(f\"✓ peft: {peft.__version__}\")\n",
        "    \n",
        "    # Check modeling_layers\n",
        "    try:\n",
        "        from transformers import modeling_layers\n",
        "        print(\"✓ transformers.modeling_layers: Available\")\n",
        "    except:\n",
        "        try:\n",
        "            from transformers.modeling_utils import modeling_layers\n",
        "            print(\"✓ transformers.modeling_layers: Available (via modeling_utils)\")\n",
        "        except:\n",
        "            print(\"⚠ transformers.modeling_layers: Not found\")\n",
        "            print(\"  This may cause issues. Try restarting runtime and running Cell 2.\")\n",
        "    \n",
        "    print(\"\\n✓ All fixed! Restart the runtime (Runtime → Restart runtime) and continue.\")\n",
        "except Exception as e:\n",
        "    print(f\"✗ Error: {e}\")\n",
        "    print(\"\\nPlease restart the runtime and run Cell 2 again.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1.1 Troubleshooting Version Compatibility\n",
        "\n",
        "If you encounter `ModuleNotFoundError: No module named 'transformers.modeling_layers'`, try one of these solutions:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If you still get the modeling_layers error after running Cell 2, run this cell:\n",
        "\n",
        "# Solution: Force reinstall transformers from source\n",
        "print(\"Force reinstalling transformers from source...\")\n",
        "!pip uninstall -y transformers\n",
        "!pip install -q git+https://github.com/huggingface/transformers.git --no-deps\n",
        "!pip install -q \"transformers[torch]\" --upgrade\n",
        "\n",
        "# Verify installations\n",
        "print(\"\\nVerifying installations...\")\n",
        "try:\n",
        "    import transformers\n",
        "    from transformers import modeling_layers\n",
        "    print(f\"✓ transformers version: {transformers.__version__}\")\n",
        "    print(f\"✓ transformers.modeling_layers imported successfully!\")\n",
        "except ImportError as e:\n",
        "    print(f\"✗ Error: {e}\")\n",
        "    print(\"Trying alternative: installing transformers 4.44.0+\")\n",
        "    !pip install -q \"transformers>=4.44.0\" --force-reinstall\n",
        "\n",
        "try:\n",
        "    import peft\n",
        "    print(f\"✓ peft version: {peft.__version__}\")\n",
        "except ImportError as e:\n",
        "    print(f\"✗ peft import error: {e}\")\n",
        "    !pip install -q \"peft>=0.18.0\" --force-reinstall\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "# Set CUDA_LAUNCH_BLOCKING for better error messages during debugging\n",
        "os.environ.setdefault(\"CUDA_LAUNCH_BLOCKING\", \"1\")\n",
        "\n",
        "import sys\n",
        "import json\n",
        "import base64\n",
        "import io\n",
        "from pathlib import Path\n",
        "from typing import Dict, List\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import (\n",
        "    AutoProcessor,\n",
        "    AutoTokenizer,\n",
        "    Qwen2_5_VLForConditionalGeneration,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "ENV_PROJECT_ROOT = os.environ.get(\"REX_OMNI_ROOT\")\n",
        "DEFAULT_PROJECT_ROOT = Path(\"/content/Rex-Omni\") if IN_COLAB else Path.cwd()\n",
        "\n",
        "candidate_roots = []\n",
        "if ENV_PROJECT_ROOT:\n",
        "    candidate_roots.append(Path(ENV_PROJECT_ROOT))\n",
        "candidate_roots.append(DEFAULT_PROJECT_ROOT)\n",
        "candidate_roots.append(Path.cwd())\n",
        "\n",
        "project_root = None\n",
        "for candidate in candidate_roots:\n",
        "    candidate_path = Path(candidate).expanduser().resolve()\n",
        "    if candidate_path.exists():\n",
        "        project_root = candidate_path\n",
        "        break\n",
        "\n",
        "if project_root is None:\n",
        "    raise FileNotFoundError(\n",
        "        \"Unable to locate the Rex-Omni project root. Set the REX_OMNI_ROOT env var or run from within the repo.\"\n",
        "    )\n",
        "\n",
        "os.chdir(str(project_root))\n",
        "\n",
        "# Add project root to path for local imports\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.insert(0, str(project_root))\n",
        "if str(project_root / \"finetuning\") not in sys.path:\n",
        "    sys.path.insert(0, str(project_root / \"finetuning\"))\n",
        "\n",
        "print(f\"Project root: {project_root}\")\n",
        "print(f\"Current directory: {os.getcwd()}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"CUDA not detected. Training/inference will fall back to CPU unless a GPU is attached.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ### 1.2 Upload VRSBench Data\n",
        "\n",
        "# Upload the `vrsbench_val_data.parquet` file to Colab. You can either:\n",
        "# - Use the file upload widget below\n",
        "# - Upload to Google Drive and mount it\n",
        "# - Use `gdown` if the file is on Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option 1: Upload file directly using file widget (Colab only)\n",
        "if IN_COLAB:\n",
        "    from google.colab import files  # type: ignore\n",
        "    import shutil\n",
        "\n",
        "    # Uncomment to upload file:\n",
        "    # uploaded = files.upload()\n",
        "    # for filename in uploaded.keys():\n",
        "    #     shutil.move(filename, '/content/Rex-Omni/vrsbench_val_data.parquet')\n",
        "    #     print(f'Moved {filename} to /content/Rex-Omni/vrsbench_val_data.parquet')\n",
        "\n",
        "    # Option 2: If file is in Google Drive\n",
        "    # from google.colab import drive\n",
        "    # drive.mount('/content/drive')\n",
        "    # !cp /content/drive/MyDrive/vrsbench_val_data.parquet /content/Rex-Omni/\n",
        "\n",
        "    # Option 3: Download from URL (if available)\n",
        "    # !wget -O /content/Rex-Omni/vrsbench_val_data.parquet <URL_TO_FILE>\n",
        "else:\n",
        "    print(\"Google Colab utilities are not available in this runtime. \")\n",
        "    print(\"Ensure `vrsbench_val_data.parquet` exists locally at `project_root / vrsbench_val_data.parquet` before proceeding.\")\n",
        "\n",
        "print(\"Make sure vrsbench_val_data.parquet is in /content/Rex-Omni/ or your local project root.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Import Dependencies\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load and Explore VRSBench Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the parquet file\n",
        "parquet_path = project_root / \"vrsbench_val_data.parquet\"\n",
        "df = pd.read_parquet(parquet_path)\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "print(f\"\\nFirst row sample:\")\n",
        "print(df.iloc[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample 1000 rows\n",
        "num_samples = 1000\n",
        "df_sample = df.sample(n=min(num_samples, len(df)), random_state=42).reset_index(drop=True)\n",
        "print(f\"Sampled {len(df_sample)} samples from {len(df)} total samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explore the data structure\n",
        "sample_row = df_sample.iloc[0]\n",
        "print(\"Sample data structure:\")\n",
        "print(f\"Image ID: {sample_row['image_id']}\")\n",
        "print(f\"Image Path: {sample_row['image_path']}\")\n",
        "print(f\"Caption: {sample_row['caption'][:100]}...\")\n",
        "print(f\"\\nObjects (first object):\")\n",
        "objects = json.loads(sample_row['objects'])\n",
        "if len(objects) > 0:\n",
        "    print(json.dumps(objects[0], indent=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Prepare VRSBench Data for JSON Training\n",
        "\n",
        "We now train directly from a JSON manifest that lists image paths, bounding boxes, and labels. The legacy TSV pipeline has been removed to avoid format drift—only the JSON export below is required.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Legacy TSV conversion has been removed in favor of the JSON dataset pipeline.\n",
        "# This cell is intentionally left empty to avoid accidental use of the old format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert VRSBench samples into the JSON manifest consumed by GroundingJsonDataset.\n",
        "# (Legacy TSV generation has been removed to avoid format drift.)\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "from PIL import ImageDraw\n",
        "import random\n",
        "\n",
        "output_dir = project_root / \"vrsbench_data\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "json_path = output_dir / \"train.json\"\n",
        "synthetic_debug_path = output_dir / \"debug_synthetic_samples.json\"\n",
        "\n",
        "print(f\"Converting to JSON dataset at {json_path}...\")\n",
        "skipped_reasons = defaultdict(int)\n",
        "dataset_list: List[Dict] = []\n",
        "\n",
        "possible_image_roots = [\n",
        "    project_root,\n",
        "    project_root / \"Images_validation\" / \"Images_val\",\n",
        "    project_root / \"Images_validation\",\n",
        "]\n",
        "\n",
        "\n",
        "def resolve_image_path(relative_path: str) -> str | None:\n",
        "    if os.path.isabs(relative_path) and os.path.exists(relative_path):\n",
        "        return relative_path\n",
        "    for root in possible_image_roots:\n",
        "        candidate = root / relative_path\n",
        "        if candidate.exists():\n",
        "            return str(candidate)\n",
        "    fallback = project_root / Path(relative_path).name\n",
        "    return str(fallback) if fallback.exists() else None\n",
        "\n",
        "\n",
        "def normalize_boxes(objects_json, width: int, height: int) -> tuple[list[list[float]], list[str]]:\n",
        "    boxes, labels = [], []\n",
        "    for obj in objects_json:\n",
        "        coords = obj.get(\"obj_coord\")\n",
        "        if not coords or len(coords) != 4:\n",
        "            continue\n",
        "        x0, y0, x1, y1 = coords\n",
        "        boxes.append([x0 * width, y0 * height, x1 * width, y1 * height])\n",
        "        labels.append(obj.get(\"obj_cls\", \"object\"))\n",
        "    return boxes, labels\n",
        "\n",
        "\n",
        "def build_record(idx: int, row: pd.Series) -> Dict | None:\n",
        "    image_path = row.get(\"image_path\", \"\")\n",
        "    resolved_path = resolve_image_path(image_path)\n",
        "    if resolved_path is None:\n",
        "        skipped_reasons[\"missing_image\"] += 1\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        with Image.open(resolved_path).convert(\"RGB\") as img:\n",
        "            width, height = img.size\n",
        "            if min(width, height) < 28:\n",
        "                skipped_reasons[\"image_too_small\"] += 1\n",
        "                return None\n",
        "    except Exception as exc:  # noqa: BLE001\n",
        "        skipped_reasons[\"image_load_failed\"] += 1\n",
        "        print(f\"Warning: failed to open {resolved_path}: {exc}\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        objects = row[\"objects\"]\n",
        "        if isinstance(objects, str):\n",
        "            objects = json.loads(objects)\n",
        "    except Exception as exc:  # noqa: BLE001\n",
        "        skipped_reasons[\"objects_parse_failed\"] += 1\n",
        "        print(f\"Warning: failed to parse objects for idx={idx}: {exc}\")\n",
        "        return None\n",
        "\n",
        "    boxes, labels = normalize_boxes(objects, width, height)\n",
        "    if not boxes:\n",
        "        skipped_reasons[\"no_boxes\"] += 1\n",
        "        return None\n",
        "\n",
        "    return {\n",
        "        \"image_path\": str(resolved_path),\n",
        "        \"boxes\": boxes,\n",
        "        \"labels\": labels,\n",
        "        \"image_id\": row.get(\"image_id\", f\"row_{idx}\"),\n",
        "        \"source\": \"vrsbench\",\n",
        "    }\n",
        "\n",
        "\n",
        "def build_synthetic_debug_samples(num_samples: int = 2) -> List[Dict]:\n",
        "    print(f\"Building {num_samples} synthetic debug sample(s)...\")\n",
        "    synthetic_samples = []\n",
        "    for s_idx in range(num_samples):\n",
        "        img = Image.new(\"RGB\", (512, 512), color=(30 + s_idx * 40, 30, 60))\n",
        "        draw = ImageDraw.Draw(img)\n",
        "        x0, y0 = random.randint(20, 150), random.randint(20, 150)\n",
        "        w, h = random.randint(150, 320), random.randint(150, 320)\n",
        "        x1, y1 = min(511, x0 + w), min(511, y0 + h)\n",
        "        draw.rectangle([x0, y0, x1, y1], outline=\"yellow\", width=4)\n",
        "        draw.text((x0 + 5, y0 + 5), \"synthetic\", fill=\"white\")\n",
        "        img_path = output_dir / f\"synthetic_debug_{s_idx}.png\"\n",
        "        img.save(img_path)\n",
        "        synthetic_samples.append(\n",
        "            {\n",
        "                \"image_path\": str(img_path),\n",
        "                \"boxes\": [[float(x0), float(y0), float(x1), float(y1)]],\n",
        "                \"labels\": [\"synthetic object\"],\n",
        "                \"image_id\": f\"synthetic_{s_idx}\",\n",
        "                \"source\": \"synthetic_debug\",\n",
        "                \"created_at\": datetime.now(datetime.timezone.utc).isoformat(),\n",
        "            }\n",
        "        )\n",
        "    return synthetic_samples\n",
        "\n",
        "\n",
        "for idx, row in tqdm(df_sample.iterrows(), total=len(df_sample), desc=\"Converting to JSON\"):\n",
        "    record = build_record(idx, row)\n",
        "    if record is not None:\n",
        "        dataset_list.append(record)\n",
        "\n",
        "if not dataset_list:\n",
        "    print(\"No valid samples found. Consider verifying the parquet file path or image root.\")\n",
        "\n",
        "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(dataset_list, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"Saved {len(dataset_list)} samples to {json_path}\")\n",
        "if skipped_reasons:\n",
        "    print(\"Skipped samples breakdown:\")\n",
        "    for reason, count in skipped_reasons.items():\n",
        "        print(f\"  - {reason}: {count}\")\n",
        "\n",
        "synthetic_samples = build_synthetic_debug_samples()\n",
        "with open(synthetic_debug_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(synthetic_samples, f, indent=2)\n",
        "print(f\"Synthetic debug samples saved to {synthetic_debug_path} (not included in training by default).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Set Up LoRA Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=16,  # LoRA rank\n",
        "    lora_alpha=32,  # LoRA alpha (scaling factor)\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ],  # Target modules for LoRA\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        ")\n",
        "\n",
        "print(\"LoRA Configuration:\")\n",
        "print(lora_config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Load Model and Apply LoRA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "model_name = \"IDEA-Research/Rex-Omni\"\n",
        "cache_dir = None\n",
        "\n",
        "print(f\"Loading model: {model_name}\")\n",
        "\n",
        "# Load model\n",
        "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "    model_name,\n",
        "    cache_dir=cache_dir,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(\"Model loaded successfully\")\n",
        "\n",
        "# Apply LoRA\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Enable gradient checkpointing to save memory\n",
        "model.gradient_checkpointing_enable()\n",
        "model.config.use_cache = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load processor and tokenizer\n",
        "processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    cache_dir=cache_dir,\n",
        "    model_max_length=4096,\n",
        "    padding_side=\"right\",\n",
        "    use_fast=False,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(\"Processor and tokenizer loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Set Up Dataset and Data Collator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import dataset classes from finetuning module\n",
        "from dataset.json_dataset import GroundingJsonDataset\n",
        "from dataset.collator import DataCollatorForSupervisedDataset\n",
        "from dataset.task_fns import GroundingTaskFn\n",
        "from dataset.task_fns.task_prompts.grounding_task import GROUNDING_SINGLE_REGION_STAGE_XYXY\n",
        "from engine.argument import DataArguments\n",
        "\n",
        "# Set up data arguments\n",
        "data_args = DataArguments()\n",
        "data_args.image_processor = processor.image_processor\n",
        "data_args.model_type = \"qwen2.5vl\"\n",
        "\n",
        "# Image size constraints\n",
        "min_pixels = 16 * 28 * 28\n",
        "max_pixels = 2560 * 28 * 28\n",
        "\n",
        "# Create task function\n",
        "task_fn = GroundingTaskFn(\n",
        "    task_prompts=GROUNDING_SINGLE_REGION_STAGE_XYXY,\n",
        "    image_min_pixels=min_pixels,\n",
        "    image_max_pixels=max_pixels,\n",
        ")\n",
        "\n",
        "print(\"Task function created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dataset\n",
        "train_dataset = GroundingJsonDataset(\n",
        "    json_file=str(json_path),\n",
        "    tokenizer=tokenizer,\n",
        "    data_args=data_args,\n",
        "    image_min_pixels=min_pixels,\n",
        "    image_max_pixels=max_pixels,\n",
        "    task_fn=task_fn,\n",
        "    system_message=\"You are a helpful assistant.\",\n",
        "    ori_box_format=\"xyxy\",\n",
        "    dataset_name=\"vrsbench_1000\",\n",
        "    max_length=4096,\n",
        ")\n",
        "\n",
        "print(f\"Dataset created with {len(train_dataset)} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick dataset sanity check to catch grid/pixel mismatches early\n",
        "import random\n",
        "from collections import Counter\n",
        "\n",
        "def dataset_health_check(dataset, sample_size: int = 12):\n",
        "    if len(dataset) == 0:\n",
        "        print(\"Dataset is empty. Please verify the JSON export step above.\")\n",
        "        return {\"summary\": Counter(), \"reasons\": Counter()}\n",
        "\n",
        "    indices = random.sample(range(len(dataset)), min(sample_size, len(dataset)))\n",
        "    summary = Counter()\n",
        "    reasons = Counter()\n",
        "\n",
        "    for idx in indices:\n",
        "        try:\n",
        "            sample = dataset[idx]\n",
        "            pixel_values = sample.get(\"pixel_values\")\n",
        "            grid_thw = sample.get(\"image_grid_thw\")\n",
        "            if pixel_values is None or grid_thw is None:\n",
        "                raise ValueError(\"missing pixel/grid data\")\n",
        "            if isinstance(grid_thw, list) and len(grid_thw) > 0 and isinstance(grid_thw[0], torch.Tensor):\n",
        "                grid_tokens = int(grid_thw[0][0].item())\n",
        "                patch_tokens = int(pixel_values.shape[0])\n",
        "                if grid_tokens != patch_tokens:\n",
        "                    raise ValueError(\n",
        "                        f\"grid tokens ({grid_tokens}) != pixel patches ({patch_tokens})\"\n",
        "                    )\n",
        "            summary[\"ok\"] += 1\n",
        "        except Exception as exc:  # noqa: BLE001\n",
        "            summary[\"failed\"] += 1\n",
        "            reasons[str(exc).split(\"\\n\")[0]] += 1\n",
        "\n",
        "    print(\"Dataset health check summary:\")\n",
        "    print(f\"  OK samples: {summary['ok']}\")\n",
        "    print(f\"  Failed samples: {summary['failed']}\")\n",
        "    if reasons:\n",
        "        print(\"  Failure reasons:\")\n",
        "        for reason, count in reasons.items():\n",
        "            print(f\"    - {reason}: {count}\")\n",
        "\n",
        "    return {\"summary\": summary, \"reasons\": reasons}\n",
        "\n",
        "train_dataset_health = dataset_health_check(train_dataset)\n",
        "print(\"Internal dataset stats:\", train_dataset.get_health_report())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create data collator\n",
        "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
        "\n",
        "print(\"Data collator created\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Set Up Training Arguments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training arguments\n",
        "output_dir = project_root / \"work_dirs\" / \"rexomni_lora_vrsbench_1000\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Adjust batch size based on Colab GPU (T4, V100, A100, etc.)\n",
        "# T4: batch_size=1, V100: batch_size=2, A100: batch_size=4\n",
        "gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "if gpu_memory_gb < 16:\n",
        "    batch_size = 1\n",
        "    grad_accum = 8\n",
        "elif gpu_memory_gb < 32:\n",
        "    batch_size = 2\n",
        "    grad_accum = 4\n",
        "else:\n",
        "    batch_size = 4\n",
        "    grad_accum = 2\n",
        "\n",
        "print(f\"GPU Memory: {gpu_memory_gb:.2f} GB\")\n",
        "print(f\"Using batch_size={batch_size}, gradient_accumulation_steps={grad_accum}\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=str(output_dir),\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    gradient_accumulation_steps=grad_accum,\n",
        "    learning_rate=2e-4,  # Higher LR for LoRA\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.03,\n",
        "    weight_decay=0.01,\n",
        "    max_grad_norm=1.0,\n",
        "    bf16=torch.cuda.is_bf16_supported(),  # Use bfloat16 if supported\n",
        "    fp16=not torch.cuda.is_bf16_supported(),  # Fallback to fp16\n",
        "    logging_steps=10,\n",
        "    save_steps=100,\n",
        "    save_total_limit=3,\n",
        "    eval_strategy=\"no\",\n",
        "    save_strategy=\"steps\",\n",
        "    gradient_checkpointing=True,\n",
        "    dataloader_num_workers=2,  # Reduce for Colab\n",
        "    remove_unused_columns=False,\n",
        "    report_to=\"none\",  # Set to \"wandb\" if you want to use Weights & Biases\n",
        "    run_name=\"rexomni_lora_vrsbench_1000\",\n",
        ")\n",
        "\n",
        "print(\"Training arguments configured\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Create Trainer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    data_collator=data_collator,\n",
        "    processing_class=tokenizer,\n",
        ")\n",
        "\n",
        "print(\"Trainer created\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Start Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start training\n",
        "print(\"Starting training...\")\n",
        "print(f\"Output directory: {output_dir}\")\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Batch size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
        "print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(\"Dataset stats before training:\", train_dataset.get_health_report())\n",
        "\n",
        "try:\n",
        "    trainer.train()\n",
        "except RuntimeError as e:  # noqa: BLE001\n",
        "    if \"indexSelectLargeIndex\" in str(e) or \"device-side assert\" in str(e):\n",
        "        print(\"\\n⚠ Training hit a CUDA indexing/device assert error.\")\n",
        "        print(\"Re-run the dataset health check cell above and inspect the reported failure reasons.\")\n",
        "        print(\"You can also lower `image_max_pixels` or filter problematic entries in the JSON export.\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Save the Fine-tuned Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the final model\n",
        "final_model_dir = output_dir / \"final_model\"\n",
        "os.makedirs(final_model_dir, exist_ok=True)\n",
        "\n",
        "# Save LoRA adapters\n",
        "model.save_pretrained(str(final_model_dir))\n",
        "\n",
        "# Save tokenizer and processor\n",
        "tokenizer.save_pretrained(str(final_model_dir))\n",
        "processor.save_pretrained(str(final_model_dir))\n",
        "\n",
        "print(f\"Model saved to: {final_model_dir}\")\n",
        "\n",
        "# Optionally save to Google Drive for persistence\n",
        "# Uncomment the following lines to save to Google Drive:\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# drive_model_dir = Path(\"/content/drive/MyDrive/rexomni_lora_vrsbench_1000\")\n",
        "# import shutil\n",
        "# shutil.copytree(final_model_dir, drive_model_dir, dirs_exist_ok=True)\n",
        "# print(f\"Model also saved to Google Drive: {drive_model_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Load and Test the Fine-tuned Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the base model\n",
        "base_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Load LoRA adapters\n",
        "from peft import PeftModel\n",
        "fine_tuned_model = PeftModel.from_pretrained(base_model, str(final_model_dir))\n",
        "\n",
        "# Merge LoRA weights (optional, for faster inference)\n",
        "merged_model = fine_tuned_model.merge_and_unload()\n",
        "\n",
        "print(\"Fine-tuned model loaded successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save merged model for use with RexOmniWrapper\n",
        "merged_model_dir = output_dir / \"merged_model\"\n",
        "os.makedirs(merged_model_dir, exist_ok=True)\n",
        "merged_model.save_pretrained(str(merged_model_dir))\n",
        "tokenizer.save_pretrained(str(merged_model_dir))\n",
        "processor.save_pretrained(str(merged_model_dir))\n",
        "\n",
        "print(f\"Merged model saved to: {merged_model_dir}\")\n",
        "\n",
        "# Download the model (optional - for Colab)\n",
        "print(\"\\nTo download the model, run:\")\n",
        "print(\"from google.colab import files\")\n",
        "print(\"import shutil\")\n",
        "print(f\"shutil.make_archive('rexomni_lora_model', 'zip', '{merged_model_dir}')\")\n",
        "print(\"files.download('rexomni_lora_model.zip')\")\n",
        "\n",
        "print(\"\\nYou can now use this model with RexOmniWrapper:\")\n",
        "print(f\"rex = RexOmniWrapper(model_path='{merged_model_dir}', backend='transformers')\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Single Test Inference\n",
        "\n",
        "Run a single test inference to verify the fine-tuned model works correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Single Test Inference using RexOmniWrapper\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import json\n",
        "from rex_omni import RexOmniWrapper\n",
        "from transformers import AutoProcessor\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"SINGLE TEST INFERENCE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Setup paths\n",
        "model_path = str(final_model_dir) if 'final_model_dir' in locals() else str(output_dir / \"final_model\")\n",
        "base_model_path = \"IDEA-Research/Rex-Omni\"\n",
        "\n",
        "# Check if merged model exists, otherwise use final_model_dir\n",
        "merged_model_path = str(output_dir / \"merged_model\")\n",
        "if os.path.exists(merged_model_path):\n",
        "    model_path = merged_model_path\n",
        "    print(f\"Using merged model: {model_path}\")\n",
        "else:\n",
        "    print(f\"Using LoRA model: {model_path}\")\n",
        "\n",
        "# Check CUDA availability and health\n",
        "print(f\"\\nCUDA available: {torch.cuda.is_available()}\")\n",
        "cuda_healthy = False\n",
        "if torch.cuda.is_available():\n",
        "    try:\n",
        "        # Test CUDA with a simple operation\n",
        "        test_tensor = torch.zeros(1).cuda()\n",
        "        del test_tensor\n",
        "        torch.cuda.synchronize()\n",
        "        cuda_healthy = True\n",
        "        print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "        print(\"✓ CUDA is healthy\")\n",
        "    except RuntimeError as e:\n",
        "        if \"device-side assert\" in str(e) or \"CUDA error\" in str(e):\n",
        "            print(\"⚠ CUDA is available but in a bad state (likely from previous error)\")\n",
        "            print(\"  Will use CPU instead. To fix CUDA, restart the Python kernel.\")\n",
        "            cuda_healthy = False\n",
        "        else:\n",
        "            raise\n",
        "else:\n",
        "    print(\"CUDA not available, will use CPU\")\n",
        "\n",
        "# Load model using RexOmniWrapper\n",
        "print(\"\\nLoading fine-tuned model with RexOmniWrapper...\")\n",
        "try:\n",
        "    rex = RexOmniWrapper(\n",
        "        model_path=model_path,\n",
        "        backend=\"transformers\",\n",
        "        max_tokens=2048,\n",
        "        temperature=0.0,\n",
        "        top_p=0.05,\n",
        "        top_k=1,\n",
        "        repetition_penalty=1.05,\n",
        "        attn_implementation=\"sdpa\",  # Use sdpa for compatibility (works on both CPU and CUDA)\n",
        "    )\n",
        "    print(\"✓ Model loaded successfully\")\n",
        "    \n",
        "    # CRITICAL: Ensure model is on CPU before resizing embeddings\n",
        "    # (RexOmniWrapper might have already moved it to CUDA)\n",
        "    print(\"\\nEnsuring model is on CPU for safe embedding resize...\")\n",
        "    try:\n",
        "        # Check current device\n",
        "        if hasattr(rex.model, 'model'):\n",
        "            first_param = next(rex.model.model.parameters())\n",
        "            current_device = first_param.device\n",
        "            print(f\"  Current model device: {current_device}\")\n",
        "            \n",
        "            if current_device.type == 'cuda':\n",
        "                print(\"  Moving model to CPU for safe embedding resize...\")\n",
        "                rex.model = rex.model.cpu()\n",
        "                # Clear any CUDA errors\n",
        "                if torch.cuda.is_available():\n",
        "                    try:\n",
        "                        torch.cuda.synchronize()\n",
        "                        torch.cuda.empty_cache()\n",
        "                    except:\n",
        "                        pass  # Ignore CUDA errors during cleanup\n",
        "                print(\"  ✓ Model moved to CPU\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ⚠ Warning during device check: {e}\")\n",
        "        # Try to force CPU anyway\n",
        "        try:\n",
        "            rex.model = rex.model.cpu()\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    # FIX VOCABULARY MISMATCH FIRST (before moving to device to avoid CUDA errors)\n",
        "    print(\"\\nFixing vocabulary mismatch (critical - must be done on CPU)...\")\n",
        "    model_vocab_size = rex.model.config.text_config.vocab_size\n",
        "    tokenizer_vocab_size = len(rex.processor.tokenizer)\n",
        "    \n",
        "    print(f\"Model vocab size: {model_vocab_size}\")\n",
        "    print(f\"Tokenizer vocab size: {tokenizer_vocab_size}\")\n",
        "    \n",
        "    if tokenizer_vocab_size > model_vocab_size:\n",
        "        print(f\"⚠ Tokenizer vocab ({tokenizer_vocab_size}) > Model vocab ({model_vocab_size})\")\n",
        "        print(\"Resizing embedding layers to match tokenizer...\")\n",
        "        \n",
        "        # Resize INPUT embeddings\n",
        "        old_input_embedding = rex.model.get_input_embeddings()\n",
        "        old_input_weight = old_input_embedding.weight.data.clone()\n",
        "        \n",
        "        from torch.nn import Embedding, Linear\n",
        "        new_input_embedding = Embedding(\n",
        "            tokenizer_vocab_size,\n",
        "            old_input_embedding.embedding_dim,\n",
        "            padding_idx=old_input_embedding.padding_idx if hasattr(old_input_embedding, 'padding_idx') else None\n",
        "        )\n",
        "        \n",
        "        # Copy old weights\n",
        "        new_input_embedding.weight.data[:model_vocab_size] = old_input_weight\n",
        "        \n",
        "        # Initialize new tokens with mean of existing embeddings\n",
        "        mean_embedding = old_input_weight.mean(dim=0)\n",
        "        new_input_embedding.weight.data[model_vocab_size:] = mean_embedding.unsqueeze(0).expand(\n",
        "            tokenizer_vocab_size - model_vocab_size, -1\n",
        "        )\n",
        "        \n",
        "        # CRITICAL: Preserve image token embeddings if they exist\n",
        "        # Image tokens might be at specific IDs that need special handling\n",
        "        if hasattr(rex.processor.tokenizer, 'image_start_id'):\n",
        "            img_start_id = rex.processor.tokenizer.image_start_id\n",
        "            if img_start_id < model_vocab_size:\n",
        "                # Image token already exists, preserve its embedding\n",
        "                new_input_embedding.weight.data[img_start_id] = old_input_weight[img_start_id]\n",
        "            elif img_start_id < tokenizer_vocab_size:\n",
        "                # Image token is in the new range, initialize it properly\n",
        "                new_input_embedding.weight.data[img_start_id] = old_input_weight.mean(dim=0)\n",
        "        \n",
        "        if hasattr(rex.processor.tokenizer, 'image_end_id'):\n",
        "            img_end_id = rex.processor.tokenizer.image_end_id\n",
        "            if img_end_id < model_vocab_size:\n",
        "                new_input_embedding.weight.data[img_end_id] = old_input_weight[img_end_id]\n",
        "            elif img_end_id < tokenizer_vocab_size:\n",
        "                new_input_embedding.weight.data[img_end_id] = old_input_weight.mean(dim=0)\n",
        "        \n",
        "        rex.model.set_input_embeddings(new_input_embedding)\n",
        "        print(f\"  ✓ Input embedding resized from {model_vocab_size} to {tokenizer_vocab_size}\")\n",
        "        \n",
        "        # Resize OUTPUT embeddings (lm_head) - CRITICAL!\n",
        "        if hasattr(rex.model, 'lm_head'):\n",
        "            old_lm_head = rex.model.lm_head\n",
        "            old_lm_head_weight = old_lm_head.weight.data.clone()\n",
        "            \n",
        "            new_lm_head = Linear(\n",
        "                old_lm_head.in_features,\n",
        "                tokenizer_vocab_size,\n",
        "                bias=old_lm_head.bias is not None\n",
        "            )\n",
        "            \n",
        "            # Copy old weights\n",
        "            new_lm_head.weight.data[:model_vocab_size] = old_lm_head_weight\n",
        "            \n",
        "            # Initialize new tokens with mean of existing weights\n",
        "            mean_weight = old_lm_head_weight.mean(dim=0)\n",
        "            new_lm_head.weight.data[model_vocab_size:] = mean_weight.unsqueeze(0).expand(\n",
        "                tokenizer_vocab_size - model_vocab_size, -1\n",
        "            )\n",
        "            \n",
        "            # Copy bias if exists\n",
        "            if old_lm_head.bias is not None:\n",
        "                old_bias = old_lm_head.bias.data.clone()\n",
        "                new_lm_head.bias.data[:model_vocab_size] = old_bias\n",
        "                mean_bias = old_bias.mean()\n",
        "                new_lm_head.bias.data[model_vocab_size:] = mean_bias\n",
        "            \n",
        "            rex.model.lm_head = new_lm_head\n",
        "            print(f\"  ✓ Output embedding (lm_head) resized from {model_vocab_size} to {tokenizer_vocab_size}\")\n",
        "        \n",
        "        # Update config\n",
        "        rex.model.config.text_config.vocab_size = tokenizer_vocab_size\n",
        "        rex.model.config.vocab_size = tokenizer_vocab_size\n",
        "        \n",
        "        print(f\"✓ All embedding layers resized from {model_vocab_size} to {tokenizer_vocab_size}\")\n",
        "    elif tokenizer_vocab_size < model_vocab_size:\n",
        "        print(f\"✓ Tokenizer vocab ({tokenizer_vocab_size}) <= Model vocab ({model_vocab_size}) - OK\")\n",
        "    else:\n",
        "        print(\"✓ Vocab sizes match!\")\n",
        "    \n",
        "    # IMPORTANT: Ensure processor uses the model's tokenizer (not a mismatched one)\n",
        "    # The processor must match the model's tokenizer for image tokens to work correctly\n",
        "    print(\"\\nVerifying processor-tokenizer compatibility...\")\n",
        "    try:\n",
        "        # Check if processor tokenizer matches model's expected tokenizer\n",
        "        # The processor should use the same tokenizer that the model was trained with\n",
        "        processor_vocab_size = len(rex.processor.tokenizer)\n",
        "        model_vocab_size_after_resize = rex.model.config.text_config.vocab_size\n",
        "        \n",
        "        if processor_vocab_size != model_vocab_size_after_resize:\n",
        "            print(f\"⚠ Processor vocab ({processor_vocab_size}) != Model vocab ({model_vocab_size_after_resize})\")\n",
        "            print(\"  This is expected after resizing - processor should still work\")\n",
        "        \n",
        "        # Verify image token IDs are valid\n",
        "        if hasattr(rex.processor.tokenizer, 'image_start_id') and hasattr(rex.processor.tokenizer, 'image_end_id'):\n",
        "            img_start_id = rex.processor.tokenizer.image_start_id\n",
        "            img_end_id = rex.processor.tokenizer.image_end_id\n",
        "            if img_start_id >= model_vocab_size_after_resize or img_end_id >= model_vocab_size_after_resize:\n",
        "                print(f\"⚠ Image token IDs ({img_start_id}, {img_end_id}) exceed vocab size!\")\n",
        "                print(\"  This might cause image token insertion issues\")\n",
        "            else:\n",
        "                print(f\"✓ Image token IDs valid: start={img_start_id}, end={img_end_id}\")\n",
        "        \n",
        "        print(\"✓ Processor-tokenizer compatibility verified\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Could not verify processor compatibility: {e}\")\n",
        "    \n",
        "    # Patch processor and embedding to clamp token IDs as safety measure\n",
        "    # BUT: Don't clamp image placeholder tokens - they need to be preserved\n",
        "    _original_call = rex.processor.__call__\n",
        "    def patched_call(*args, **kwargs):\n",
        "        result = _original_call(*args, **kwargs)\n",
        "        if isinstance(result, dict) and 'input_ids' in result:\n",
        "            if isinstance(result['input_ids'], torch.Tensor):\n",
        "                actual_vocab_size = rex.model.config.text_config.vocab_size\n",
        "                # Get image token IDs if they exist\n",
        "                img_token_ids = set()\n",
        "                if hasattr(rex.processor.tokenizer, 'image_start_id'):\n",
        "                    img_token_ids.add(rex.processor.tokenizer.image_start_id)\n",
        "                if hasattr(rex.processor.tokenizer, 'image_end_id'):\n",
        "                    img_token_ids.add(rex.processor.tokenizer.image_end_id)\n",
        "                if hasattr(rex.processor.tokenizer, 'image_pad_id'):\n",
        "                    img_token_ids.add(rex.processor.tokenizer.image_pad_id)\n",
        "                \n",
        "                # Clamp non-image tokens, but preserve image tokens\n",
        "                input_ids = result['input_ids']\n",
        "                mask = torch.ones_like(input_ids, dtype=torch.bool)\n",
        "                for img_id in img_token_ids:\n",
        "                    if img_id < actual_vocab_size:\n",
        "                        mask = mask & (input_ids != img_id)\n",
        "                \n",
        "                # Only clamp non-image tokens\n",
        "                clamped_ids = torch.clamp(input_ids, 0, actual_vocab_size - 1)\n",
        "                result['input_ids'] = torch.where(mask, clamped_ids, input_ids)\n",
        "        return result\n",
        "    rex.processor.__call__ = patched_call\n",
        "    \n",
        "    # Patch embedding layer forward\n",
        "    original_embedding_forward = rex.model.get_input_embeddings().forward\n",
        "    def safe_embedding_forward(input_ids):\n",
        "        actual_vocab_size = rex.model.config.text_config.vocab_size\n",
        "        # Preserve image tokens if they exist\n",
        "        img_token_ids = set()\n",
        "        if hasattr(rex.processor.tokenizer, 'image_start_id'):\n",
        "            img_token_ids.add(rex.processor.tokenizer.image_start_id)\n",
        "        if hasattr(rex.processor.tokenizer, 'image_end_id'):\n",
        "            img_token_ids.add(rex.processor.tokenizer.image_end_id)\n",
        "        if hasattr(rex.processor.tokenizer, 'image_pad_id'):\n",
        "            img_token_ids.add(rex.processor.tokenizer.image_pad_id)\n",
        "        \n",
        "        # Clamp non-image tokens\n",
        "        mask = torch.ones_like(input_ids, dtype=torch.bool)\n",
        "        for img_id in img_token_ids:\n",
        "            if img_id < actual_vocab_size:\n",
        "                mask = mask & (input_ids != img_id)\n",
        "        \n",
        "        clamped_ids = torch.clamp(input_ids, 0, actual_vocab_size - 1)\n",
        "        safe_input_ids = torch.where(mask, clamped_ids, input_ids)\n",
        "        return original_embedding_forward(safe_input_ids)\n",
        "    rex.model.get_input_embeddings().forward = safe_embedding_forward\n",
        "    print(\"✓ Processor and embedding layer patched for safety (preserving image tokens)\")\n",
        "    \n",
        "    # Now move model to device (vocabulary is fixed)\n",
        "    # Use CUDA only if it's healthy, otherwise use CPU\n",
        "    device = torch.device(\"cuda\" if (torch.cuda.is_available() and cuda_healthy) else \"cpu\")\n",
        "    print(f\"\\nMoving model to {device}...\")\n",
        "    \n",
        "    # Clear CUDA cache if available and healthy\n",
        "    if device.type == \"cuda\" and cuda_healthy:\n",
        "        try:\n",
        "            torch.cuda.empty_cache()\n",
        "            torch.cuda.synchronize()\n",
        "            print(\"  CUDA cache cleared\")\n",
        "        except RuntimeError as e:\n",
        "            if \"device-side assert\" in str(e) or \"CUDA error\" in str(e):\n",
        "                print(\"  ⚠ CUDA error during cache clear, switching to CPU\")\n",
        "                device = torch.device(\"cpu\")\n",
        "                cuda_healthy = False\n",
        "            else:\n",
        "                raise\n",
        "    \n",
        "    # Change attention implementation if needed\n",
        "    if device.type == \"cpu\" or not cuda_healthy:\n",
        "        print(\"⚠ Changing attention implementation to sdpa (CPU or no CUDA)...\")\n",
        "        rex.model.config._attn_implementation = \"sdpa\"\n",
        "        if hasattr(rex.model.config, 'text_config'):\n",
        "            rex.model.config.text_config._attn_implementation = \"sdpa\"\n",
        "        if hasattr(rex.model.config, 'vision_config'):\n",
        "            rex.model.config.vision_config._attn_implementation = \"sdpa\"\n",
        "        \n",
        "        if hasattr(rex.model, 'visual') and hasattr(rex.model.visual, 'blocks'):\n",
        "            for block in rex.model.visual.blocks:\n",
        "                if hasattr(block, 'attn') and hasattr(block.attn, 'config'):\n",
        "                    block.attn.config._attn_implementation = \"sdpa\"\n",
        "        \n",
        "        if hasattr(rex.model, 'model') and hasattr(rex.model.model, 'layers'):\n",
        "            for layer in rex.model.model.layers:\n",
        "                if hasattr(layer, 'self_attn') and hasattr(layer.self_attn, 'config'):\n",
        "                    layer.self_attn.config._attn_implementation = \"sdpa\"\n",
        "        print(\"✓ Attention implementation changed to sdpa\")\n",
        "    \n",
        "    # Move model to device with error handling\n",
        "    try:\n",
        "        # Set model to eval mode before moving (safer)\n",
        "        rex.model.eval()\n",
        "        \n",
        "        # Move model to device\n",
        "        rex.model = rex.model.to(device)\n",
        "        print(f\"✓ Model moved to {device}\")\n",
        "        \n",
        "        # Verify device placement\n",
        "        if hasattr(rex.model, 'model'):\n",
        "            first_param = next(rex.model.model.parameters())\n",
        "            print(f\"✓ Model device verified: {first_param.device}\")\n",
        "            \n",
        "    except RuntimeError as e:\n",
        "        if \"CUDA\" in str(e) or \"device-side assert\" in str(e):\n",
        "            print(f\"⚠ CUDA error during model move: {e}\")\n",
        "            print(\"  This might be due to vocabulary mismatch or CUDA state issues.\")\n",
        "            print(\"  Trying to clear CUDA cache and retry...\")\n",
        "            \n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "                torch.cuda.synchronize()\n",
        "            \n",
        "            # Try moving again\n",
        "            try:\n",
        "                rex.model = rex.model.to(device)\n",
        "                print(f\"✓ Model moved to {device} on retry\")\n",
        "            except Exception as e2:\n",
        "                print(f\"✗ Failed to move model to {device}: {e2}\")\n",
        "                print(\"  Falling back to CPU...\")\n",
        "                device = torch.device(\"cpu\")\n",
        "                rex.model = rex.model.to(device)\n",
        "                print(f\"✓ Model moved to CPU as fallback\")\n",
        "        else:\n",
        "            raise\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"✗ Error loading model: {e}\")\n",
        "    raise\n",
        "\n",
        "# Get a test sample from the dataset\n",
        "print(\"\\nPreparing test sample...\")\n",
        "test_idx = 0\n",
        "test_row = df_sample.iloc[test_idx]\n",
        "test_image_path = test_row['image_path']\n",
        "\n",
        "# Handle image path\n",
        "if not os.path.isabs(test_image_path):\n",
        "    possible_paths = [\n",
        "        os.path.join(project_root, test_image_path),\n",
        "        os.path.join(str(project_root), test_image_path),\n",
        "        test_image_path\n",
        "    ]\n",
        "    for path in possible_paths:\n",
        "        if os.path.exists(path):\n",
        "            test_image_path = path\n",
        "            break\n",
        "    else:\n",
        "        print(f\"⚠ Warning: Image not found at {test_image_path}\")\n",
        "        print(\"Trying to use first available image from dataset...\")\n",
        "        # Try to find any image\n",
        "        for idx, row in df_sample.iterrows():\n",
        "            img_path = row['image_path']\n",
        "            for root in [str(project_root), \"\"]:\n",
        "                full_path = os.path.join(root, img_path) if root else img_path\n",
        "                if os.path.exists(full_path):\n",
        "                    test_image_path = full_path\n",
        "                    test_row = row\n",
        "                    break\n",
        "            if os.path.exists(test_image_path):\n",
        "                break\n",
        "\n",
        "# Load test image\n",
        "try:\n",
        "    test_image = Image.open(test_image_path).convert(\"RGB\")\n",
        "    print(f\"✓ Test image loaded: {test_image_path}\")\n",
        "    print(f\"  Image size: {test_image.size}\")\n",
        "except Exception as e:\n",
        "    print(f\"✗ Error loading image: {e}\")\n",
        "    raise\n",
        "\n",
        "# Extract categories from the test sample\n",
        "try:\n",
        "    objs = json.loads(test_row['objects']) if isinstance(test_row['objects'], str) else test_row['objects']\n",
        "    categories = list(set([o.get('obj_cls', 'object') for o in objs if isinstance(o, dict)]))\n",
        "    if not categories:\n",
        "        categories = [\"object\"]\n",
        "    print(f\"  Categories to detect: {categories}\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠ Could not extract categories: {e}\")\n",
        "    categories = [\"object\"]\n",
        "\n",
        "# Test processor image token insertion before inference\n",
        "print(\"\\nTesting processor image token insertion...\")\n",
        "try:\n",
        "    # Create a test message with image (same format RexOmniWrapper uses)\n",
        "    test_messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": test_image},\n",
        "                {\"type\": \"text\", \"text\": \"test\"}\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    # Apply chat template\n",
        "    text = rex.processor.apply_chat_template(\n",
        "        test_messages, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "    \n",
        "    # Process with processor\n",
        "    test_inputs = rex.processor(\n",
        "        text=[text],\n",
        "        images=[test_image],\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    if \"image_grid_thw\" in test_inputs:\n",
        "        print(f\"  Processor grid_thw: {test_inputs['image_grid_thw'][0].tolist()}\")\n",
        "    \n",
        "    # Check for image tokens\n",
        "    input_ids = test_inputs['input_ids'][0]\n",
        "    \n",
        "    # Check for image placeholder tokens\n",
        "    img_token_count = 0\n",
        "    if hasattr(rex.processor.tokenizer, 'image_start_id'):\n",
        "        img_start_id = rex.processor.tokenizer.image_start_id\n",
        "        img_end_id = getattr(rex.processor.tokenizer, 'image_end_id', None)\n",
        "        img_pad_id = getattr(rex.processor.tokenizer, 'image_pad_id', None)\n",
        "        \n",
        "        img_token_count = (input_ids == img_start_id).sum().item()\n",
        "        if img_end_id:\n",
        "            img_token_count += (input_ids == img_end_id).sum().item()\n",
        "        if img_pad_id:\n",
        "            img_token_count += (input_ids == img_pad_id).sum().item()\n",
        "        \n",
        "        print(f\"  Image token IDs: start={img_start_id}, end={img_end_id}, pad={img_pad_id}\")\n",
        "        print(f\"  Image tokens found in input_ids: {img_token_count}\")\n",
        "        \n",
        "        if img_token_count == 0:\n",
        "            print(\"  ⚠ WARNING: No image tokens found! This will cause the error.\")\n",
        "            print(\"  The processor might not be inserting image tokens correctly.\")\n",
        "            print(\"  This could be due to processor-tokenizer mismatch.\")\n",
        "            raise RuntimeError(\n",
        "                \"Processor did not insert any <|vision|> tokens; aborting inference before model.generate to avoid CUDA asserts.\"\n",
        "            )\n",
        "        else:\n",
        "            print(f\"  ✓ Image tokens are being inserted correctly ({img_token_count} found)\")\n",
        "    else:\n",
        "        print(\"  ⚠ Could not find image token IDs in tokenizer\")\n",
        "        print(\"  This might indicate a processor configuration issue\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"  ⚠ Error testing processor: {e}\")\n",
        "\n",
        "# Run inference\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Running inference...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "try:\n",
        "    results = rex.inference(\n",
        "        images=test_image,\n",
        "        task=\"detection\",\n",
        "        categories=categories\n",
        "    )\n",
        "    \n",
        "    result = results[0] if isinstance(results, list) else results\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"INFERENCE RESULT\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Success: {result.get('success', False)}\")\n",
        "    print(f\"Inference time: {result.get('inference_time', 0):.2f}s\")\n",
        "    print(f\"Output tokens: {result.get('num_output_tokens', 0)}\")\n",
        "    print(f\"\\nRaw output:\\n{result.get('raw_output', 'N/A')}\")\n",
        "    \n",
        "    # Extract predictions\n",
        "    preds = result.get('extracted_predictions', {})\n",
        "    if preds:\n",
        "        print(f\"\\nExtracted predictions:\")\n",
        "        total_boxes = 0\n",
        "        for cat, boxes in preds.items():\n",
        "            print(f\"  {cat}: {len(boxes)} box(es)\")\n",
        "            total_boxes += len(boxes)\n",
        "        print(f\"  Total: {total_boxes} boxes\")\n",
        "        \n",
        "        # Visualize the result\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
        "        ax.imshow(test_image)\n",
        "        ax.axis('off')\n",
        "        \n",
        "        # Draw bounding boxes\n",
        "        colors = plt.cm.tab10(np.linspace(0, 1, len(preds)))\n",
        "        color_idx = 0\n",
        "        for cat, boxes in preds.items():\n",
        "            for box in boxes:\n",
        "                if isinstance(box, dict) and 'bbox' in box:\n",
        "                    bbox = box['bbox']\n",
        "                    if len(bbox) == 4:\n",
        "                        x0, y0, x1, y1 = bbox\n",
        "                        rect = patches.Rectangle(\n",
        "                            (x0, y0), x1 - x0, y1 - y0,\n",
        "                            linewidth=2, \n",
        "                            edgecolor=colors[color_idx % len(colors)], \n",
        "                            facecolor='none',\n",
        "                            label=cat\n",
        "                        )\n",
        "                        ax.add_patch(rect)\n",
        "                        ax.text(x0, y0 - 5, cat, color=colors[color_idx % len(colors)], \n",
        "                               fontsize=10, weight='bold', \n",
        "                               bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))\n",
        "            color_idx += 1\n",
        "        \n",
        "        ax.set_title(f\"Test Inference Result\\nCategories: {', '.join(categories)}\", fontsize=12)\n",
        "        if preds:\n",
        "            ax.legend(loc='upper right', fontsize=8)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"\\n⚠ No predictions extracted from output\")\n",
        "        print(\"This might be normal if the model output format differs from expected\")\n",
        "        \n",
        "        # Still show the image\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
        "        ax.imshow(test_image)\n",
        "        ax.axis('off')\n",
        "        ax.set_title(f\"Test Image\\n(No boxes detected)\\nCategories: {', '.join(categories)}\", fontsize=12)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"✓ Test inference completed successfully!\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "except Exception as e:\n",
        "    error_msg = str(e)\n",
        "    print(f\"\\n✗ Inference failed: {error_msg}\")\n",
        "    \n",
        "    if \"token\" in error_msg.lower() or \"vocab\" in error_msg.lower():\n",
        "        print(\"\\nTroubleshooting tips:\")\n",
        "        print(\"1. The tokenizer vocabulary might not match the model\")\n",
        "        print(\"2. Try using the base model's tokenizer (already attempted)\")\n",
        "        print(\"3. Check if the model was saved correctly with its tokenizer\")\n",
        "        print(\"4. You may need to re-save the model with the correct tokenizer\")\n",
        "    \n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook has:\n",
        "1. ✅ Set up the environment for Google Colab\n",
        "2. ✅ Loaded 1000 samples from VRSBench dataset\n",
        "3. ✅ Converted data to TSV format required for training\n",
        "4. ✅ Set up LoRA configuration for parameter-efficient fine-tuning\n",
        "5. ✅ Fine-tuned the Rex-Omni model with LoRA\n",
        "6. ✅ Saved the fine-tuned model\n",
        "\n",
        "### Important Notes for Colab:\n",
        "- **Session Timeout**: Colab sessions timeout after inactivity. Save your work frequently!\n",
        "- **GPU Runtime**: Make sure you're using a GPU runtime (Runtime → Change runtime type → GPU)\n",
        "- **Data Persistence**: Files in `/content` are deleted when the session ends. Save important files to Google Drive or download them.\n",
        "- **Memory**: If you run out of memory, reduce `batch_size` or `gradient_accumulation_steps`\n",
        "\n",
        "### Next Steps:\n",
        "- Evaluate the model on validation data\n",
        "- Adjust hyperparameters (learning rate, LoRA rank, etc.)\n",
        "- Train for more epochs if needed\n",
        "- Download or save the model to Google Drive for persistence\n",
        "- Use the fine-tuned model for inference\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

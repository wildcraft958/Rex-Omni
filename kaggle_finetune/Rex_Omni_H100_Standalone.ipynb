{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rex-Omni H100 Standalone Fine-tuning\n",
    "\n",
    "This notebook is a complete, standalone solution for fine-tuning the **Rex-Omni** model (based on Qwen2.5-VL) using LoRA/QLoRA.\n",
    "\n",
    "**Features:**\n",
    "- **H100 Optimized**: Uses `Flash Attention 2` and `bfloat16` for maximum speed on H100/A100 GPUs.\n",
    "- **Standalone**: Includes data conversion, setup, and training in one place.\n",
    "- **Rex-Omni Specific**: Handles the specific grounding format required by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation\n",
    "Install necessary libraries. We use `flash-attn` for H100 acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q torch>=2.4.0 transformers>=4.46.0 peft>=0.13.0 bitsandbytes>=0.44.0 accelerate>=1.0.0 qwen-vl-utils datasets wandb scipy\n",
    "%pip install -q flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "Set your parameters here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# CONFIGURATION\n",
    "MODEL_ID = \"IDEA-Research/Rex-Omni\"\n",
    "DATA_PATH = \"/kaggle/input/your-dataset/metadata.json\" # CHANGE THIS to your metadata file path\n",
    "IMAGE_FOLDER = \"/kaggle/input/your-dataset/images\"     # CHANGE THIS to your image folder path\n",
    "OUTPUT_DIR = \"/kaggle/working/rex-omni-finetuned\"\n",
    "\n",
    "# Training Hyperparameters\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 4          # H100 has 80GB, can handle larger batches\n",
    "GRAD_ACCUM = 2          # Adjust based on memory\n",
    "LORA_RANK = 64\n",
    "USE_QLORA = True        # Set False for full bf16 training if memory allows (faster on H100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation Helper\n",
    "This function converts your metadata JSON into the specific JSONL format required by Rex-Omni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "def normalize_bbox(bbox, width, height):\n",
    "    x0, y0, x1, y1 = bbox\n",
    "    x0_n = max(0.0, min(1.0, x0 / width))\n",
    "    y0_n = max(0.0, min(1.0, y0 / height))\n",
    "    x1_n = max(0.0, min(1.0, x1 / width))\n",
    "    y1_n = max(0.0, min(1.0, y1 / height))\n",
    "    \n",
    "    x0_bin = int(x0_n * 999)\n",
    "    y0_bin = int(y0_n * 999)\n",
    "    x1_bin = int(x1_n * 999)\n",
    "    y1_bin = int(y1_n * 999)\n",
    "    return f\"<{x0_bin}><{y0_bin}><{x1_bin}><{y1_bin}>\"\n",
    "\n",
    "def prepare_data(image_folder, metadata_file, output_file):\n",
    "    print(f\"Converting data from {metadata_file}...\")\n",
    "    image_folder_path = Path(image_folder)\n",
    "    \n",
    "    with open(metadata_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    with open(output_file, 'w') as f_out:\n",
    "        for item in tqdm(data):\n",
    "            image_filename = item.get('image')\n",
    "            if not image_filename: continue\n",
    "                \n",
    "            image_path = image_folder_path / image_filename\n",
    "            if not image_path.exists(): continue\n",
    "            \n",
    "            try:\n",
    "                with Image.open(image_path) as img:\n",
    "                    width, height = img.size\n",
    "            except: continue\n",
    "\n",
    "            objects = item.get('objects', [])\n",
    "            if not objects: continue\n",
    "\n",
    "            cat_to_boxes = {}\n",
    "            for obj in objects:\n",
    "                cat = obj['category']\n",
    "                bbox = obj['bbox']\n",
    "                if cat not in cat_to_boxes: cat_to_boxes[cat] = []\n",
    "                cat_to_boxes[cat].append(bbox)\n",
    "            \n",
    "            answer_parts = []\n",
    "            for cat, boxes in cat_to_boxes.items():\n",
    "                box_tokens = [normalize_bbox(b, width, height) for b in boxes]\n",
    "                box_str = \",\".join(box_tokens)\n",
    "                part = f\"<|object_ref_start|>{cat}<|object_ref_end|><|box_start|>{box_str}<|box_end|>\"\n",
    "                answer_parts.append(part)\n",
    "            \n",
    "            answer_text = \", \".join(answer_parts)\n",
    "            \n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": str(image_path.absolute())},\n",
    "                    {\"type\": \"text\", \"text\": \"<image>\\nDetect the objects in this image.\"}\n",
    "                ]},\n",
    "                {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": answer_text}]}\n",
    "            ]\n",
    "            f_out.write(json.dumps({\"messages\": messages}) + '\\n')\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "\n",
    "# Run conversion\n",
    "JSONL_PATH = \"/kaggle/working/train_data.jsonl\"\n",
    "# Uncomment the line below if you have uploaded data\n",
    "# prepare_data(IMAGE_FOLDER, DATA_PATH, JSONL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Logic\n",
    "This section initializes the model, applies LoRA, and runs the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor, TrainingArguments, Trainer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from datasets import load_dataset\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "def train():\n",
    "    # 1. Load Processor\n",
    "    processor = AutoProcessor.from_pretrained(MODEL_ID, min_pixels=256*28*28, max_pixels=1280*28*28)\n",
    "\n",
    "    # 2. Load Dataset\n",
    "    # Ensure JSONL_PATH exists (run prepare_data first)\n",
    "    if not os.path.exists(JSONL_PATH):\n",
    "        print(f\"Dataset not found at {JSONL_PATH}. Please run data preparation.\")\n",
    "        return\n",
    "        \n",
    "    dataset = load_dataset(\"json\", data_files=JSONL_PATH, split=\"train\")\n",
    "\n",
    "    # 3. Collator\n",
    "    def collate_fn(examples):\n",
    "        texts = [processor.apply_chat_template(example[\"messages\"], tokenize=False, add_generation_prompt=True) for example in examples]\n",
    "        image_inputs, video_inputs = process_vision_info(examples)\n",
    "        inputs = processor(text=texts, images=image_inputs, videos=video_inputs, padding=True, return_tensors=\"pt\")\n",
    "        labels = inputs[\"input_ids\"].clone()\n",
    "        labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "        image_token_id = processor.tokenizer.convert_tokens_to_ids(\"<|image_pad|>\")\n",
    "        labels[labels == image_token_id] = -100\n",
    "        inputs[\"labels\"] = labels\n",
    "        return inputs\n",
    "\n",
    "    # 4. Model Init\n",
    "    bnb_config = None\n",
    "    if USE_QLORA:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "\n",
    "    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        quantization_config=bnb_config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        attn_implementation=\"flash_attention_2\", # H100 Optimization\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    if USE_QLORA:\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # 5. LoRA Config\n",
    "    peft_config = LoraConfig(\n",
    "        r=LORA_RANK,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\"\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    # 6. Training Args\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRAD_ACCUM,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        learning_rate=2e-4,\n",
    "        bf16=True, # H100 Optimization\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"epoch\",\n",
    "        report_to=\"none\",\n",
    "        remove_unused_columns=False,\n",
    "        gradient_checkpointing=True,\n",
    "        dataloader_pin_memory=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(model=model, args=training_args, train_dataset=dataset, data_collator=collate_fn)\n",
    "    trainer.train()\n",
    "    trainer.save_model(OUTPUT_DIR)\n",
    "    processor.save_pretrained(OUTPUT_DIR)\n",
    "    print(f\"Model saved to {OUTPUT_DIR}\")\n",
    "\n",
    "# Start Training\n",
    "# train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
